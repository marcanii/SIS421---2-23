{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las librerias que vamos a utilizar en el notebook\n",
    "\n",
    "import torch # libreria principal\n",
    "import torch.nn as nn # libreria para redes neuronales\n",
    "import torch.optim as optim # libreria para optimizacion\n",
    "from torch.utils.data import DataLoader, Dataset # libreria para cargar los datos\n",
    "import torchvision.transforms as transforms # libreria para transformar los datos\n",
    "from datasets import load_dataset # libreria para cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos el dataset de Tiny ImageNet y lo guardamos en la variable dataset\n",
    "dataset = load_dataset(\"zh-plus/tiny-imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostramos la estructura del dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una transformación para el preprocesamiento de imágenes.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3), # pasar a escala de grises (3 canales)\n",
    "    transforms.Resize((64, 64)), # redimensionar a 64x64\n",
    "    transforms.ToTensor(), # convertir a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # normalizar\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina su clase de conjunto de datos personalizado\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, split, transform=None):\n",
    "        self.X = dataset[split]['image'] # cargar imágenes\n",
    "        self.y = dataset[split]['label'] # cargar etiquetas\n",
    "        self.transform = transform\n",
    "\n",
    "    # Devuelve el número de muestras en el conjunto de datos.\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # Devuelve una muestra del conjunto de datos en la posición index.\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cree instancias de su conjunto de datos personalizado para divisiones de entrenamiento y prueba\n",
    "train_dataset = CustomDataset(dataset, 'train', transform=transform)\n",
    "test_dataset = CustomDataset(dataset, 'valid', transform=transform)\n",
    "\n",
    "# Crear cargadores de datos para cargar los datos.\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usar getitem para obtener una muestra del train_loader\n",
    "img_sample, label_sample = train_loader.dataset[0]\n",
    "img_sample.shape, label_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(c_in, c_out, k=3, p=1, s=1, pk=2, ps=2):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(c_in, c_out, k, padding=p, stride=s),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(pk, stride=ps)\n",
    "    )\n",
    "\n",
    "def block2(c_in, c_out):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(c_in, c_out),\n",
    "        torch.nn.ReLU()\n",
    "    )\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "  def __init__(self, n_channels=1, n_outputs=10):\n",
    "    super().__init__()\n",
    "    self.conv1 = block(n_channels, 64)\n",
    "    self.conv2 = block(64, 128)\n",
    "    # self.conv3 = block(128, 256)\n",
    "    self.fc = torch.nn.Linear(128 * 7 * 7, n_outputs)\n",
    "\n",
    "  def forward(self, x):\n",
    "    print(\"Dimensiones:\")\n",
    "    print(\"Entrada: \", x.shape)\n",
    "    x = self.conv1(x)\n",
    "    print(\"conv1: \", x.shape)\n",
    "    x = self.conv2(x)\n",
    "    print(\"conv2: \", x.shape)\n",
    "    # x = self.conv3(x)\n",
    "    # print(\"conv3: \", x.shape)\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    print(\"pre fc: \", x.shape)\n",
    "    x = self.fc(x)\n",
    "    print(\"Salida: \", x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "\n",
    "output = model(torch.randn(64, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "  def __init__(self, n_channels=1, n_outputs=10):\n",
    "    super().__init__()\n",
    "    self.conv1 = block(n_channels, 392)\n",
    "    self.conv2 = block(392, 196)\n",
    "    self.fc = torch.nn.Linear(196*7*7, n_outputs)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def fit(model, dataloader, epochs=5):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss, train_acc = [], []\n",
    "        bar = tqdm(dataloader['train'])\n",
    "        for batch in bar:\n",
    "            X, y = batch\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
    "            train_acc.append(acc)\n",
    "            bar.set_description(f\"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}\")\n",
    "        bar = tqdm(dataloader['test'])\n",
    "        val_loss, val_acc = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in bar:\n",
    "                X, y = batch\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_hat = model(X)\n",
    "                loss = criterion(y_hat, y)\n",
    "                val_loss.append(loss.item())\n",
    "                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
    "                val_acc.append(acc)\n",
    "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
    "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "fit(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_user",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
