{"cells":[{"cell_type":"markdown","metadata":{"id":"Yg90deR13qYE"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/039_nlp_transfer/nlp_transfer.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"hiDRydOc3qYF"},"source":["# Clasificación de texto - Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"TI2sYEw53qYG"},"source":["En el [post](https://sensioai.com/blog/038_clasificacion_texto) anterior vimos cómo podemos entrenar una `red neuronal recurrente` para clasificar texto. En este tipo de tarea, nuestro modelo será capaz de asignar una etiqueta concreta entre varias a una pieza de texto determinada. Vimos, por ejemplo, que podemos saber de manera automática si una opinión de una película es positiva o negativa. En este post vamos a resolver exactamente el mismo caso, pero introduciendo una nueva técnica muy utilizada: el `transfer learning`."]},{"cell_type":"markdown","metadata":{"id":"IJjxLfcx3qYG"},"source":["## Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"9mEuD8t93qYH"},"source":["Esta técnica nos permite entrenar redes neuronales de manera más rápida, con menores requisitos computacionales y permitiendo el entrenamiento de redes con mejores prestaciones con pequeños datasets. La idea consiste en entrenar una red neuronal en un gran dataset, con grandes recursos computacionales, y una vez entrenada utilizar el conocimiento que este modelo ya posee como punto de partida para nuestro caso particular en el proceso conocido como `fine tuning`.\n","\n","![](https://pennylane.ai/qml/_images/transfer_learning_general.png)\n","\n","Este proceso de `fine tuning` puede variar según la tarea, pero lo más común es sustituir las capas finales de la red por nuevas capas adaptadas a nuestra tarea y entrenar solo estas nuevas capas, dejando intactas las capas ya entrenadas. Sin embargo, en el caso en el que los datos usados en la nueva tarea sean muy diferentes que los usados originalmente, también es común el entrenamiento de toda la red, a partir de los pesos pre-entrenados.\n","\n","Como comentábamos al principio, esta técnica es muy utilizada en la práctica. Podemos encontrar modelos pre-entrenados en diferentes librerías, que podemos descargar y empezar a utilizar directamente. El `transfer learning` es utilizado tanto en aplicaciones de lenguaje como tareas visuales, y lo usaremos de manera extensiva de ahora en adelante."]},{"cell_type":"markdown","metadata":{"id":"YkprYd7j3qYH"},"source":[" ## El *dataset*"]},{"cell_type":"markdown","metadata":{"id":"l14aVDhe3qYH"},"source":["Seguimos utilizando el dataset IMDB, disponible en [torchtext](https://pytorch.org/text/)."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":364},"executionInfo":{"elapsed":4549,"status":"ok","timestamp":1686886694788,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"PDsy3nLaQ4T5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchtext==0.1.1 in /usr/local/lib/python3.10/dist-packages (0.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.1.1) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.1.1) (4.65.0)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchtext==0.1.1) (1.26.15)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchtext==0.1.1) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchtext==0.1.1) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etorchtext==0.1.1) (3.4)\n"]}],"source":["!pip install torchtext==0.1.1"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"elapsed":4565,"status":"error","timestamp":1686886662503,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"XSI0LuJw3qYI","outputId":"8b705842-c71e-45ed-9c9f-ac4289f4b40b"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-2-3464deba8a20\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 3\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import torch\n","import torchtext\n","from torchtext.legacy import data\n","from torchtext.legacy import datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"aborted","timestamp":1686886662503,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"2HbseR3y3qYK"},"outputs":[],"source":["TEXT = data.Field(tokenize = 'spacy')\n","LABEL = data.LabelField(dtype = torch.long)\n","\n","train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"aborted","timestamp":1686886662504,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"n-t5qGVr3qYN"},"outputs":[],"source":["len(train_data), len(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"aborted","timestamp":1686886662505,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"8TWnH0iI3qYP"},"outputs":[],"source":["print(vars(train_data.examples[0]))"]},{"cell_type":"markdown","metadata":{"id":"qmURBdC33qYR"},"source":["## *Embeddings* pre-entrenados"]},{"cell_type":"markdown","metadata":{"id":"lujcFj4H3qYR"},"source":["El primer ejemplo de `transfer learning` que vamos a ver es el uso de `embeddings` pre-entrenados. Recuerda que un embedding es la respresentación vectorial de cada palabra en el vocabulario que utilizaremos para alimentar nuestra red recurrente. Puedes aprender más sobre `embeddings` en este [post](https://sensioai.com/blog/037_charRNN). En `torchtext` podemos descargar estos `embeddings` en la función `build_vocab`, con el parámtero `vectors`. En la documentación encontrarás los diferentes `embeddings` disponibles."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1686886662506,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"6N7gwB8F3qYS"},"outputs":[],"source":["MAX_VOCAB_SIZE = 10000\n","\n","TEXT.build_vocab(train_data,\n","                 max_size = MAX_VOCAB_SIZE,\n","                 vectors = \"glove.6B.100d\", # embeddings pre-entrenados\n","                 unk_init = torch.Tensor.normal_)\n","\n","LABEL.build_vocab(train_data)\n","\n","len(TEXT.vocab), len(LABEL.vocab)"]},{"cell_type":"markdown","metadata":{"id":"BHEbZ90n3qYW"},"source":["Y, de la misma manera que hicimos en el post anterior, definimos nuestros *dataloaders* con la clase `torchtext.data.BucketIterator`."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1686886662506,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"adNca2SR3qYW"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","dataloader = {\n","    'train': data.BucketIterator(train_data, batch_size=64, shuffle=True, sort_within_batch=True, device=device),\n","    'test': data.BucketIterator(test_data, batch_size=64, device=device)\n","}"]},{"cell_type":"markdown","metadata":{"id":"PW7QtEu33qYZ"},"source":["## El modelo"]},{"cell_type":"markdown","metadata":{"id":"psaait293qYZ"},"source":["Usaremos exactamente el mismo modelo que ya vimos en el post anterior. Este modelo está compuesto, principalmente, por la capa `embedding`, que en este caso sustituiremos por los vectores descargados anteriormente, y las capas recurrente y lineal, que entrenaremos desde cero."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1686886662507,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"bnAt6VZJ3qYZ"},"outputs":[],"source":["class RNN(torch.nn.Module):\n","    def __init__(self, input_dim, embedding_dim=128, hidden_dim=128, output_dim=2, num_layers=2, dropout=0.2, bidirectional=False):\n","        super().__init__()\n","        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n","        self.rnn = torch.nn.GRU(\n","            input_size=embedding_dim,\n","            hidden_size=hidden_dim,\n","            num_layers=num_layers,\n","            dropout=dropout if num_layers \u003e 1 else 0,\n","            bidirectional=bidirectional\n","        )\n","        self.fc = torch.nn.Linear(2*hidden_dim if bidirectional else hidden_dim, output_dim)\n","\n","    def forward(self, text):\n","        # no entrenamos los embeddings\n","        with torch.no_grad():\n","            #text = [sent len, batch size]\n","            embedded = self.embedding(text)\n","        #embedded = [sent len, batch size, emb dim]\n","        output, hidden = self.rnn(embedded)\n","        #output = [sent len, batch size, hid dim]\n","        y = self.fc(output[-1,:,:].squeeze(0))\n","        return y"]},{"cell_type":"markdown","metadata":{"id":"o-aP8aVb3qYb"},"source":["Una vez definido el modelo, sustituimos los tensores en la capa `embedding` por los vectores pre-entrenados descargados anteriormente."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1686886662507,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"W0oSPUdI3qYb"},"outputs":[],"source":["model = RNN(input_dim=len(TEXT.vocab), bidirectional=True, embedding_dim=100)\n","\n","pretrained_embeddings = TEXT.vocab.vectors\n","model.embedding.weight.data.copy_(pretrained_embeddings)\n","# ponemos a cero los pesos correspondientes a los tokens \u003cunk\u003e y \u003cpad\u003e\n","model.embedding.weight.data[TEXT.vocab.stoi[TEXT.unk_token]] = torch.zeros(100)\n","model.embedding.weight.data[TEXT.vocab.stoi[TEXT.pad_token]] = torch.zeros(100)\n","\n","outputs = model(torch.randint(0, len(TEXT.vocab), (100, 64)))\n","outputs.shape"]},{"cell_type":"markdown","metadata":{"id":"R6bmOZwA3qYd"},"source":["## Entrenamiento"]},{"cell_type":"markdown","metadata":{"id":"vz6TyCjv3qYd"},"source":["Para entrenar nuestra red usamos el bucle estándar que ya usamos en posts anteriores."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1686886662508,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"dixF27i_3qYe"},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","\n","def fit(model, dataloader, epochs=5):\n","    model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","    criterion = torch.nn.CrossEntropyLoss()\n","    for epoch in range(1, epochs+1):\n","        model.train()\n","        train_loss, train_acc = [], []\n","        bar = tqdm(dataloader['train'])\n","        for batch in bar:\n","            X, y = batch\n","            X, y = X.to(device), y.to(device)\n","            optimizer.zero_grad()\n","            y_hat = model(X)\n","            loss = criterion(y_hat, y)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss.append(loss.item())\n","            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n","            train_acc.append(acc)\n","            bar.set_description(f\"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}\")\n","        bar = tqdm(dataloader['test'])\n","        val_loss, val_acc = [], []\n","        model.eval()\n","        with torch.no_grad():\n","            for batch in bar:\n","                X, y = batch\n","                X, y = X.to(device), y.to(device)\n","                y_hat = model(X)\n","                loss = criterion(y_hat, y)\n","                val_loss.append(loss.item())\n","                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n","                val_acc.append(acc)\n","                bar.set_description(f\"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}\")\n","        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1686886662508,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"6nPMXQub3qYg"},"outputs":[],"source":["fit(model, dataloader)"]},{"cell_type":"markdown","metadata":{"id":"PLDcflUq3qYi"},"source":["## Generando predicciones"]},{"cell_type":"markdown","metadata":{"id":"TV4ph12F3qYi"},"source":["Una vez nuestro modelo ha sido entrenado, podemos generar predicciones exactamente igual que hicimos en el post anterior."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1686886662509,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"WrocF-sx3qYj"},"outputs":[],"source":["import spacy\n","nlp = spacy.load('en')\n","\n","def predict(model, X):\n","    model.eval()\n","    with torch.no_grad():\n","        X = torch.tensor(X).to(device)\n","        pred = model(X)\n","        return pred"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1686886662509,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"1Z5iW-qc3qYk"},"outputs":[],"source":["sentences = [\"this film is terrible\", \"this film is great\", \"this film is good\", \"a waste good time\"]\n","tokenized = [[tok.text for tok in nlp.tokenizer(sentence)] for sentence in sentences]\n","indexed = [[TEXT.vocab.stoi[_t] for _t in t] for t in tokenized]\n","tensor = torch.tensor(indexed).permute(1,0)\n","predictions = torch.argmax(predict(model, tensor), axis=1)\n","predictions"]},{"cell_type":"markdown","metadata":{"id":"2RrkI-yW3qYn"},"source":["## Transformers"]},{"cell_type":"markdown","metadata":{"id":"qtfVMXMj3qYn"},"source":["Como has visto en el ejemplo anterior, utilizar unos `embeddings` pre-entrenados puede darnos mucho mejores resultados que entrenarlos desde cero, ya que la representación de nuestras palabras será mucho mejor desde el principio. Siguiendo en esta línea, podemos sustituir nuestra capa `embedding` por otro modelo que nos aportará todavía mejores resultados, un `transformer`.\n","\n","Estos modelos aparecieron alrededor de 2017, y fueron presentados en el famoso artículo [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf). Desde su aparación, estos modelos están batiendo todos los *benchmarks* en las diferentes tareas de procesado de lenguaje, y son utilizados como base de cualquier modelo competente a día de hoy. De momento, no entraremos en detalles en la definición de esta arquitectura (lo dejamos para un futuro post, ya que hay mucha tela que cortar) pero vamos a ver como utilizar un `transformer` para hacer `transfer learning` y obtener muy buenos resultados de manera rápida.\n","\n","Una librería muy utilizada para trabajar con estos modelos es la librería `transformers` de [huggingface](https://huggingface.co/)."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1686886662510,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"4-98jW6m3qYn"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"l799IUwE3qYp"},"source":["En primer lugar, tendremos que utilizar el mismo `tokenizer` utilizado para entrenar el modelo original. En este caso usaremos la red conocida como `BERT`."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1686886662510,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"zbdlmJg73qYp"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1686886662510,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"wAZ1S1O03qYr"},"outputs":[],"source":["tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n","tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1686886662511,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"r6T3jhIT3qYt"},"outputs":[],"source":["indexes = tokenizer.convert_tokens_to_ids(tokens)\n","indexes"]},{"cell_type":"markdown","metadata":{"id":"rvqpnWsi3qYv"},"source":["A diferencia de las `redes neuronales recurrentes`, los transformers trabajan con longitudes de secuencia fijas (no son modelos recurrentes). Es por este motivo que tenemos que asegurarnos que ninguna frase en el dataset tiene mayor longitud que la máxima permitida por `BERT`, que es de 512 palabras."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1686886662511,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"pOF7qzbb3qYv"},"outputs":[],"source":["max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n","\n","print(max_input_length)\n","\n","def tokenize_and_cut(sentence):\n","    tokens = tokenizer.tokenize(sentence)\n","    tokens = tokens[:max_input_length-2]\n","    return tokens"]},{"cell_type":"markdown","metadata":{"id":"M2O7x5VU3qYw"},"source":["`torchtext` nos da la libertad de definir nuestros propios tokenizers, y podemos incluirlos de la siguiente manera."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1686886662511,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"MbtJ6RWx3qYx"},"outputs":[],"source":["TEXT = data.Field(batch_first = True,\n","                  use_vocab = False,\n","                  tokenize = tokenize_and_cut,\n","                  preprocessing = tokenizer.convert_tokens_to_ids,\n","                  init_token = tokenizer.cls_token_id,\n","                  eos_token = tokenizer.sep_token_id,\n","                  pad_token = tokenizer.pad_token_id,\n","                  unk_token = tokenizer.unk_token_id)\n","\n","LABEL = data.LabelField(dtype = torch.long)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1686886662512,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"PtjlybRJ3qYy"},"outputs":[],"source":["train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n","\n","LABEL.build_vocab(train_data)\n","\n","dataloader = {\n","    'train': data.BucketIterator(train_data, batch_size=64, shuffle=True, sort_within_batch=True, device=device),\n","    'test': data.BucketIterator(test_data, batch_size=64, device=device)\n","}"]},{"cell_type":"markdown","metadata":{"id":"Kg-BtVHe3qYz"},"source":["Una vez tenemos los datos preparados con el nuevo tokenizer, necesitamos definir nuestro nuevo modelo. En este caso, `BERT` se encargará de actuar como nuestra capa `embedding`, proveyendo de la mejor representación posible de nuestro texto para que las siguientes capas puedan clasificarlo."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1686886662512,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"Po0hmYTQ3qY0"},"outputs":[],"source":["from transformers import BertModel\n","\n","class BERT(torch.nn.Module):\n","    def __init__(self, hidden_dim=256, output_dim=2, n_layers=2, bidirectional=True, dropout=0.2):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        # freeze BERT\n","        for name, param in self.bert.named_parameters():\n","            if name.startswith('bert'):\n","                param.requires_grad = False\n","\n","        embedding_dim = self.bert.config.to_dict()['hidden_size']\n","        self.rnn = torch.nn.GRU(embedding_dim,\n","                          hidden_dim,\n","                          num_layers = n_layers,\n","                          bidirectional = bidirectional,\n","                          batch_first = True,\n","                          dropout = 0 if n_layers \u003c 2 else dropout)\n","\n","        self.fc = torch.nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n","\n","    def forward(self, text):\n","        with torch.no_grad():\n","            embedded = self.bert(text)[0]\n","        output, hidden = self.rnn(embedded)\n","        y = self.fc(output[:,-1,:].squeeze(1))\n","        return y"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1686886662512,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"epXiRbi03qY3"},"outputs":[],"source":["model = BERT()\n","fit(model, dataloader, epochs=3)"]},{"cell_type":"markdown","metadata":{"id":"5_tLNWtFE4gC"},"source":["Puedes ver que en la primera *epoch* ya tenemos un modelo mejor que cualquiera de los obtenidos anteriormente cuando entrenamos desde cero. Y finalmente, podemos generar las predicciones de la siguiente manera"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1686886662512,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"rnp0yU6O3qY5"},"outputs":[],"source":["def predict(sentence):\n","    tokenized = [tok[:max_input_length-2] for tok in tokenizer.tokenize(sentence)]\n","    indexed = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(tokenized) + [tokenizer.sep_token_id]\n","    tensor = torch.tensor([indexed]).to(device)\n","    model.eval()\n","    return torch.argmax(model(tensor), axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1686886662513,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"4MEhc0rJG8j-"},"outputs":[],"source":["sentences = [\"Best film ever !\", \"this movie is terrible\"]\n","preds = [predict(s) for s in sentences]\n","preds"]},{"cell_type":"markdown","metadata":{"id":"Ql2zt14l3qY7"},"source":["## Resumen"]},{"cell_type":"markdown","metadata":{"id":"4Kzp2jHl3qY7"},"source":["En este post hemos visto como podemos obtener mejores modelos de clasificación de texto si utilizamos el `transfer learning`. Con esta técnica, sustituiremos las primeras capas de nuestro modelo por otros modelos ya entrenados en otros datasets. Dadas las condiciones adecuadas, esta técnica nos va a permitir entrenar modelos muy buenos con pocos datos y en poco tiempo. En nuestro caso, hemos conseguido nuestro mejor clasificador utilizando el modelo `BERT` como capa `embedding` y entrenando nuestro modelo recurrente encima, el cual solo hemos necesitado entrenar por una epoch."]}],"metadata":{"accelerator":"GPU","colab":{"name":"","provenance":[{"file_id":"https://github.com/juansensio/blog/blob/master/039_nlp_transfer/nlp_transfer.ipynb","timestamp":1638513231142}],"version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"233.594px"},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":0}