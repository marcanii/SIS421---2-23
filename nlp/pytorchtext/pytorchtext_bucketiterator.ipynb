{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorchtext_bucketiterator.ipynb","provenance":[{"file_id":"https://github.com/gmihaila/ml_things/blob/master/notebooks/pytorch/pytorchtext_bucketiterator.ipynb","timestamp":1628392797998}],"collapsed_sections":["qSuUpkj1UuUa"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ee3945502c664d018e4bc5c2d3a8bedc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0f90b515b0394d3bba8f82a84d9bdcb4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8a215d695acd430d8866322d3a2cd88f","IPY_MODEL_ca1a0083759d436b9d03c0c8969e3545"]}},"0f90b515b0394d3bba8f82a84d9bdcb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8a215d695acd430d8866322d3a2cd88f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_37a59f8553a54623b55f73d486cfac18","_dom_classes":[],"description":"pos Files: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":12500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":12500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a67b11637fe6478cb1493523ab724e07"}},"ca1a0083759d436b9d03c0c8969e3545":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_78613ce829d8470c89e6bbad8d5ffb91","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 12500/12500 [00:11&lt;00:00, 1104.07it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6c6b447c304b4989896532ca3c72c063"}},"37a59f8553a54623b55f73d486cfac18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a67b11637fe6478cb1493523ab724e07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"78613ce829d8470c89e6bbad8d5ffb91":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6c6b447c304b4989896532ca3c72c063":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"89856c4e42474e51ad25abe4af9241c0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_edbe1035852d42bab6bc345ed6c588d1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_962f4f3f47f64abba2e2fb21b4c85efd","IPY_MODEL_03ac75d7479b4d269d250ca3c651d308"]}},"edbe1035852d42bab6bc345ed6c588d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"962f4f3f47f64abba2e2fb21b4c85efd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_94f5649ace0741c88e57fbd829f4bd1a","_dom_classes":[],"description":"neg Files: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":12500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":12500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9d74f7dc45b645ee947942c3605340f1"}},"03ac75d7479b4d269d250ca3c651d308":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c40c105aa3e0463280eb3b7827622337","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 12500/12500 [00:07&lt;00:00, 1694.25it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_65a1bbf6ed4441b09be48ae3e172482b"}},"94f5649ace0741c88e57fbd829f4bd1a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9d74f7dc45b645ee947942c3605340f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c40c105aa3e0463280eb3b7827622337":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"65a1bbf6ed4441b09be48ae3e172482b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ec7102c775d54cbd945d66ec5a171dd4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_216fc6d91e954c66bbead29b70ec147d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bd9c19302c3f4f0e9978b2332df04ccb","IPY_MODEL_160deb114e9340bbb1048a01241d3d17"]}},"216fc6d91e954c66bbead29b70ec147d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bd9c19302c3f4f0e9978b2332df04ccb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a250cf546ec24b8ba318e0cb6dbcf772","_dom_classes":[],"description":"pos Files: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":12500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":12500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9f67ea09b68b4dd1ad8f396ace346058"}},"160deb114e9340bbb1048a01241d3d17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_90da6c0c39bb4c9c91af149423b0d7b4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 12500/12500 [00:03&lt;00:00, 3451.62it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0ad517e7f9fd41259d462a63244d2165"}},"a250cf546ec24b8ba318e0cb6dbcf772":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9f67ea09b68b4dd1ad8f396ace346058":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"90da6c0c39bb4c9c91af149423b0d7b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0ad517e7f9fd41259d462a63244d2165":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"72556e4fcb674d9fa50da9b8c89ddc32":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b6bedbfb36bb47a190f7d88655a65903","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_917406a6516b4fbdb4f68fd81d2fdfd1","IPY_MODEL_f899520278c24e82a2192b859bbe2fd7"]}},"b6bedbfb36bb47a190f7d88655a65903":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"917406a6516b4fbdb4f68fd81d2fdfd1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_34645434bc794e3f90758da863a4fc02","_dom_classes":[],"description":"neg Files: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":12500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":12500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c9bf0bdbe33c4346a33ea178ef75155c"}},"f899520278c24e82a2192b859bbe2fd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_715aa585526347fba26c49aa8a94066c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 12500/12500 [00:11&lt;00:00, 1084.10it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fd302f8a261346c2acea6c3bf5a0cd7c"}},"34645434bc794e3f90758da863a4fc02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c9bf0bdbe33c4346a33ea178ef75155c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"715aa585526347fba26c49aa8a94066c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fd302f8a261346c2acea6c3bf5a0cd7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0540dbfd2b304fb5a4338ac177d5c1e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_583d1b6545d642eb8be424dcc8dde698","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_17f566fc253c4ebeb031f84f71760d3b","IPY_MODEL_b4c11650b68540b1bdb2e1ce811e25aa"]}},"583d1b6545d642eb8be424dcc8dde698":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"17f566fc253c4ebeb031f84f71760d3b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3c99d7ff2b0a4ca2b3c9923e840a0dae","_dom_classes":[],"description":"pos Files: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":12500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":12500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_84f8753830a642059ebdf22dbba770e5"}},"b4c11650b68540b1bdb2e1ce811e25aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2c31069f96754d91ae60faf04dbe737a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 12500/12500 [00:07&lt;00:00, 1652.86it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_65fa71e70c4241808d2d406dcb309ef4"}},"3c99d7ff2b0a4ca2b3c9923e840a0dae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"84f8753830a642059ebdf22dbba770e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2c31069f96754d91ae60faf04dbe737a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"65fa71e70c4241808d2d406dcb309ef4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8216a3f8111242758da8e39931526df7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_080c947f50b4423e9335c989752cedd1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_848eef63256640b6947acb19030ab659","IPY_MODEL_97768141e7254083a7899cec5a3ba2c4"]}},"080c947f50b4423e9335c989752cedd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"848eef63256640b6947acb19030ab659":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dbb67e057fd74b83930b328777b9643b","_dom_classes":[],"description":"neg Files: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":12500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":12500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_78839ad5766c4f398495979c878635cf"}},"97768141e7254083a7899cec5a3ba2c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6f186dbfb3cc4c88b7e57f5d1c6abe49","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 12500/12500 [02:59&lt;00:00, 69.73it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c87d6401cf7c4eec8c39e59e8f87aeb8"}},"dbb67e057fd74b83930b328777b9643b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"78839ad5766c4f398495979c878635cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6f186dbfb3cc4c88b7e57f5d1c6abe49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c87d6401cf7c4eec8c39e59e8f87aeb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a64f5eb6b7114c4e8ca02f0c3f54b87a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d6c09a02382a41ef894e50555bcd8312","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_38a0772c85ef472c9a41aa1e8fd44d56","IPY_MODEL_7c64071a23934f86803ca12aa370a667"]}},"d6c09a02382a41ef894e50555bcd8312":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"38a0772c85ef472c9a41aa1e8fd44d56":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8a4a994e84e649b5800dcb46b9a47098","_dom_classes":[],"description":"pos Files: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":12500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":12500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c2513e3d1a1140298a3b81c6d60078d7"}},"7c64071a23934f86803ca12aa370a667":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_608f10bfedb9464fa81bb47aaf43f4d7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 12500/12500 [02:55&lt;00:00, 71.28it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1c0a648846434bf1969869a2978ba253"}},"8a4a994e84e649b5800dcb46b9a47098":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c2513e3d1a1140298a3b81c6d60078d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"608f10bfedb9464fa81bb47aaf43f4d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1c0a648846434bf1969869a2978ba253":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bfbcdddac955436d8cc11cbd99aa520f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_068bdeb7efe74371802b4f347ae6e214","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4eba572c10c04818a967f00f491ed36e","IPY_MODEL_9608b6f4cc8a41d2be2f1f4f854e1221"]}},"068bdeb7efe74371802b4f347ae6e214":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4eba572c10c04818a967f00f491ed36e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ffb22841789c4550ad1e85665ec5ad4e","_dom_classes":[],"description":"neg Files: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":12500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":12500,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2243d9f70a0a4fe1a1a19b65b8872506"}},"9608b6f4cc8a41d2be2f1f4f854e1221":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d0f850e4dcab47ff9067e1f7724aeecb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 12500/12500 [02:51&lt;00:00, 72.72it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ec501c5cbec6414cb8244ab8ff198ad4"}},"ffb22841789c4550ad1e85665ec5ad4e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2243d9f70a0a4fe1a1a19b65b8872506":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d0f850e4dcab47ff9067e1f7724aeecb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ec501c5cbec6414cb8244ab8ff198ad4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qSuUpkj1UuUa"},"source":["##### © Copyright 2020 [George Mihaila](https://github.com/gmihaila).\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");"]},{"cell_type":"code","metadata":{"id":"FkXPZsPbT2aV","executionInfo":{"status":"ok","timestamp":1628491733059,"user_tz":240,"elapsed":530,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}}},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aP1zAHX4S70e"},"source":["# **Better Batches with PyTorchText BucketIterator**\n","\n","## **How to use PyTorchText BucketIterator to sort text data for better batching.**\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/pytorchtext_bucketiterator.ipynb) &nbsp;\n","[![Generic badge](https://img.shields.io/badge/GitHub-Source-greensvg)](https://github.com/gmihaila/ml_things/blob/master/notebooks/pytorch/pytorchtext_bucketiterator.ipynb)\n","[![Generic badge](https://img.shields.io/badge/Article-Medium-black.svg)](https://gmihaila.medium.com/better-batches-with-pytorchtext-bucketiterator-12804a545e2a)\n","[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n","\n","\n","<br>\n","\n","**Disclaimer:** *The format of this tutorial notebook is very similar with my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format.*\n","\n","<br>\n","\n","This notebook is a simple tutorial on how to use the powerful **PytorchText**  **BucketIterator** functionality to group examples (**I use examples and sequences interchangeably**) of similar lengths into batches. This allows us to provide the most optimal batches when training models with text data.\n","\n","Having batches with similar length examples provides a lot of gain for recurrent models (RNN, GRU, LSTM) and transformers models (bert, roBerta, gpt2, xlnet, etc.) where padding will be minimal.\n","\n","Basically any model that takes as input variable text data sequences will benefit from this tutorial.\n","\n","**I will not train any models in this notebook!** I will release a tutorial where I use this implementation to train a transformer model.\n","\n","The purpose is to use an example text datasets and batch it using **PyTorchText** with **BucketIterator** and show how it groups text sequences of similar length in batches.\n","\n","This tutorial has two main parts:\n","\n","* **Using PyTorch Dataset with PyTorchText Bucket Iterator**: Here I implemented a standard PyTorch Dataset class that reads in the example text datasets and use PyTorch Bucket Iterator to group similar length examples in same batches. I want to show how easy it is to use this powerful functionality form PyTorchText on a regular PyTorch Dataset workflow which you already have setup.\n","\n","* **Using PyTorch Text TabularDataset with PyTorchText Bucket Iterator**: Here I use the built-in PyTorchText TabularDataset that reads data straight from local files without the need to create a PyTorch Dataset class. Then I follow same steps as in the previous part to show how nicely text examples are grouped together.\n","\n","*This notebooks is a code adaptation and implementation inspired from a few sources:* [torchtext_translation_tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html), [pytorch/text - GitHub](https://github.com/pytorch/text), [torchtext documentation](https://torchtext.readthedocs.io/en/latest/index.html#) and [A Comprehensive Introduction to Torchtext](https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/).\n","\n","<br>\n","\n","## **What should I know for this notebook?**\n","\n","Some basic PyTorch regarding Dataset class and using DataLoaders. Some knowledge of PyTorchText is helpful but not critical in understanding this tutorial. The BucketIterator is similar in applying Dataloader to a PyTorch Dataset.\n","\n","<br>\n","\n","## **How to use this notebook?**\n","\n","The code is made with reusability in mind. It can be easily adapted for other text datasets and other NLP tasks in order to achieve optimal batching. \n","\n","Comments should provide enough guidance to easily adapt this notebook to your needs.\n","\n","This code is designed mostly for **classification tasks** in mind, but it can be adapted for any other Natural Language Processing tasks where batching text data is needed.\n","\n","\n","\n","\n","\n","\n","<br>\n","\n","\n","## **Dataset**\n","\n","I will use the well known movies reviews positive - negative labeled [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).\n","\n","The description provided on the Stanford website:\n","\n","*This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details.*\n","\n","**Why this dataset?** I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with.\n","\n","<br>\n","\n","## **Coding**\n","\n","Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output.\n","\n","I made this format to be easy to follow if you decide to run each code cell in your own python notebook.\n","\n","When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.\n","\n","<br>\n"]},{"cell_type":"markdown","metadata":{"id":"8ppW60cUXZQK"},"source":["## Downloads\n","\n","Download the IMDB Movie Reviews sentiment dataset and unzip it locally."]},{"cell_type":"code","metadata":{"id":"6l_gehghXapy","executionInfo":{"status":"ok","timestamp":1628491745436,"user_tz":240,"elapsed":11971,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}}},"source":["# download the dataset\n","!wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","# unzip it\n","!tar -zxf /content/aclImdb_v1.tar.gz"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UCLtm5BiXona"},"source":["## **Installs**\n","\n","* **[ml_things](https://github.com/gmihaila/ml_things)** library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project.\n"]},{"cell_type":"code","metadata":{"id":"1JQhmThRXp7b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628491765804,"user_tz":240,"elapsed":20379,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}},"outputId":"869e7752-33d1-4d76-c138-c993f1ab31fa"},"source":["# Install helper functions.\n","!pip install -q git+https://github.com/gmihaila/ml_things.git"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 64 kB 2.4 MB/s \n","\u001b[K     |████████████████████████████████| 10.3 MB 27.0 MB/s \n","\u001b[?25h  Building wheel for ml-things (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X5IO8-xrXvWY"},"source":["## **Imports**\n","\n","Import all needed libraries for this notebook.\n","\n","Declare basic parameters used for this notebook:\n","\n","* `device` - Device to use by torch: GPU/CPU. I use CPU as default since I will not perform any costly operations.\n","\n","* `train_batch_size` - Batch size used on train data.\n","\n","* `valid_batch_size` - Batch size used for validation data. It usually is greater than `train_batch_size` since the model would only need to make prediction and no gradient calculations is needed."]},{"cell_type":"code","metadata":{"id":"J42h802BXwTe","executionInfo":{"status":"ok","timestamp":1628491768781,"user_tz":240,"elapsed":2979,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}}},"source":["import io\n","import os\n","import torchtext\n","from tqdm.notebook import tqdm\n","from ml_things import fix_text\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Will use `cpu` for simplicity.\n","device = 'cpu'\n","\n","# Number of batches for training\n","train_batch_size = 10\n","\n","# Number of batches for validation. Use a larger value than training.\n","# It helps speed up the validation process.\n","valid_batch_size = 20"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9biWe-OhS4Wo"},"source":["## Using PyTorch Dataset\n","\n","This is where I create the PyTorch Dataset objects for training and validation that **can** be used to feed data into a model. This is standard procedure when using PyTorch.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ChQWVc4IUUPb"},"source":["### Dataset Class\n","\n","Implementation of the PyTorch Dataset class.\n","\n","Most important components in a PyTorch Dataset class are:\n","* `__len__(self, )` where it returns the number of examples in our dataset that we read in `__init__(self, )`. This will ensure that `len()` will return the number of examples.\n","* `__getitem__(self, item)` where given an index `item` will return the example corresponding to the `item` position."]},{"cell_type":"code","metadata":{"id":"-Fo_0qq_S_Zf","executionInfo":{"status":"ok","timestamp":1628491768781,"user_tz":240,"elapsed":14,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}}},"source":["class MovieReviewsTextDataset(Dataset):\n","  r\"\"\"PyTorch Dataset class for loading data.\n","\n","  This is where the data parsing happens.\n","\n","  This class is built with reusability in mind.\n","\n","  Arguments:\n","\n","    path (:obj:`str`):\n","        Path to the data partition.\n","\n","  \"\"\"\n","\n","  def __init__(self, path):\n","\n","    # Check if path exists.\n","    if not os.path.isdir(path):\n","      # Raise error if path is invalid.\n","      raise ValueError('Invalid `path` variable! Needs to be a directory')\n","    \n","    self.texts = []\n","    self.labels = []\n","    # Since the labels are defined by folders with data we loop \n","    # through each label.\n","    for label  in ['pos', 'neg']:\n","      sentiment_path = os.path.join(path, label)\n","\n","      # Get all files from path.\n","      files_names = os.listdir(sentiment_path)#[:10] # Sample for debugging.\n","      # Go through each file and read its content.\n","      for file_name in tqdm(files_names, desc=f'{label} Files'):\n","        file_path = os.path.join(sentiment_path, file_name)\n","\n","        # Read content.\n","        content = io.open(file_path, mode='r', encoding='utf-8').read()\n","        # Fix any unicode issues.\n","        content = fix_text(content)\n","        # Save content.\n","        self.texts.append(content)\n","        # Save labels.\n","        self.labels.append(label)\n","\n","    # Number of examples.\n","    self.n_examples = len(self.labels)\n","\n","    return\n","\n","\n","  def __len__(self):\n","    r\"\"\"When used `len` return the number of examples.\n","\n","    \"\"\"\n","    \n","    return self.n_examples\n","\n","\n","  def __getitem__(self, item):\n","    r\"\"\"Given an index return an example from the position.\n","    \n","    Arguments:\n","\n","      item (:obj:`int`):\n","          Index position to pick an example to return.\n","\n","    Returns:\n","      :obj:`Dict[str, str]`: Dictionary of inputs that are used to feed \n","      to a model.\n","\n","    \"\"\"\n","\n","    return {'text':self.texts[item], 'label':self.labels[item]}"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52b7iTASUYAZ"},"source":["### Train - Validation Datasets\n","\n","Create PyTorch Dataset for train and validation partitions."]},{"cell_type":"code","metadata":{"id":"EQ-HrjdOUcK6","colab":{"base_uri":"https://localhost:8080/","height":303,"referenced_widgets":["ee3945502c664d018e4bc5c2d3a8bedc","0f90b515b0394d3bba8f82a84d9bdcb4","8a215d695acd430d8866322d3a2cd88f","ca1a0083759d436b9d03c0c8969e3545","37a59f8553a54623b55f73d486cfac18","a67b11637fe6478cb1493523ab724e07","78613ce829d8470c89e6bbad8d5ffb91","6c6b447c304b4989896532ca3c72c063","89856c4e42474e51ad25abe4af9241c0","edbe1035852d42bab6bc345ed6c588d1","962f4f3f47f64abba2e2fb21b4c85efd","03ac75d7479b4d269d250ca3c651d308","94f5649ace0741c88e57fbd829f4bd1a","9d74f7dc45b645ee947942c3605340f1","c40c105aa3e0463280eb3b7827622337","65a1bbf6ed4441b09be48ae3e172482b","ec7102c775d54cbd945d66ec5a171dd4","216fc6d91e954c66bbead29b70ec147d","bd9c19302c3f4f0e9978b2332df04ccb","160deb114e9340bbb1048a01241d3d17","a250cf546ec24b8ba318e0cb6dbcf772","9f67ea09b68b4dd1ad8f396ace346058","90da6c0c39bb4c9c91af149423b0d7b4","0ad517e7f9fd41259d462a63244d2165","72556e4fcb674d9fa50da9b8c89ddc32","b6bedbfb36bb47a190f7d88655a65903","917406a6516b4fbdb4f68fd81d2fdfd1","f899520278c24e82a2192b859bbe2fd7","34645434bc794e3f90758da863a4fc02","c9bf0bdbe33c4346a33ea178ef75155c","715aa585526347fba26c49aa8a94066c","fd302f8a261346c2acea6c3bf5a0cd7c"]},"executionInfo":{"status":"ok","timestamp":1628491783496,"user_tz":240,"elapsed":14726,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}},"outputId":"349eeef3-bbf7-45c5-f382-f141ba6908ba"},"source":["print('Dealing with Train...')\n","# Create pytorch dataset.\n","train_dataset = MovieReviewsTextDataset(path='/content/aclImdb/train')\n","\n","print(f'Created `train_dataset` with {len(train_dataset)} examples!')\n","\n","print()\n","\n","print('Dealing with Validation...')\n","# Create pytorch dataset.\n","valid_dataset =  MovieReviewsTextDataset(path='/content/aclImdb/test')\n","                               \n","print(f'Created `valid_dataset` with {len(valid_dataset)} examples!')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Dealing with Train...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee3945502c664d018e4bc5c2d3a8bedc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='pos Files', max=12500.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89856c4e42474e51ad25abe4af9241c0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='neg Files', max=12500.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Created `train_dataset` with 25000 examples!\n","\n","Dealing with Validation...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec7102c775d54cbd945d66ec5a171dd4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='pos Files', max=12500.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72556e4fcb674d9fa50da9b8c89ddc32","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='neg Files', max=12500.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Created `valid_dataset` with 25000 examples!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OuXCNfZLUeJJ"},"source":["### PyTorch DataLoader\n","\n","In order to group examples from the PyTorch Dataset into batches we use PyTorch DataLoader. This is standard when using PyTorch."]},{"cell_type":"code","metadata":{"id":"vW8mE9wYTfLJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628491783497,"user_tz":240,"elapsed":13,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}},"outputId":"4966af12-6e19-476d-87cc-ec791ac58088"},"source":["# Move pytorch dataset into dataloader.\n","torch_train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n","print(f'Created `torch_train_dataloader` with {len(torch_train_dataloader)} batches!')\n","\n","# Move pytorch dataset into dataloader.\n","torch_valid_dataloader = DataLoader(valid_dataset, batch_size=valid_batch_size, shuffle=False)\n","print(f'Created `torch_valid_dataloader` with {len(torch_valid_dataloader)} batches!')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Created `torch_train_dataloader` with 2500 batches!\n","Created `torch_valid_dataloader` with 1250 batches!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6IAuKwSvIbxe"},"source":["### PyTorchText Bucket Iterator Dataloader\n","\n","Here is where the magic happens! We pass in the **train_dataset** and **valid_dataset** PyTorch Dataset splits into **BucketIterator** to create the actual batches.\n","\n","It's very nice that PyTorchText can handle splits! No need to write same line of code again for train and validation split.\n","\n","**The `sort_key` parameter is very important!** It is used to order text sequences in batches. Since we want to batch sequences of text with similar length, we will use a simple function that returns the length of an data example (`len(x['text')`). This function needs to follow the format of the PyTorch Dataset we created in order to return the length of an example, in my case I return a dictionary with `text` key for an example.\n","\n","**It is important to keep `sort=False` and `sort_with_batch=True` to only sort the examples in each batch and not the examples in the whole dataset!**\n","\n","Find more details in the PyTorchText **BucketIterator** documentation [here](https://torchtext.readthedocs.io/en/latest/data.html#bucketiterator) - look at the **BPTTIterator** because it has same parameters except the **bptt_len** argument.\n","\n","**Note:**\n","*If you want just a single DataLoader use `torchtext.data.BucketIterator` instead of `torchtext.data.BucketIterator.splits` and make sure to provide just one PyTorch Dataset instead of tuple of PyTorch Datasets and change the parameter `batch_sizes` and its tuple values to `batch_size` with single value: `dataloader = torchtext.data.BucketIterator(dataset, batch_size=batch_size, )`*"]},{"cell_type":"code","metadata":{"id":"wwD9TB--weJO","executionInfo":{"status":"ok","timestamp":1628491783497,"user_tz":240,"elapsed":12,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}}},"source":["from torchtext.legacy.data import BucketIterator"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKfYOod_LQrs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628491784175,"user_tz":240,"elapsed":690,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}},"outputId":"0bc008f6-bb17-4c92-a103-b30c634efe4a"},"source":["# Group similar length text sequences together in batches.\n","torchtext_train_dataloader, torchtext_valid_dataloader = BucketIterator.splits(\n","    \n","                              # Datasets for iterator to draw data from\n","                              (train_dataset, valid_dataset),\n","\n","                              # Tuple of train and validation batch sizes.\n","                              batch_sizes=(train_batch_size, valid_batch_size),\n","\n","                              # Device to load batches on.\n","                              device=device, \n","\n","                              # Function to use for sorting examples.\n","                              sort_key=lambda x: len(x['text']),\n","\n","\n","                              # Repeat the iterator for multiple epochs.\n","                              repeat=True, \n","\n","                              # Sort all examples in data using `sort_key`.\n","                              sort=False, \n","\n","                              # Shuffle data on each epoch run.\n","                              shuffle=True,\n","\n","                              # Use `sort_key` to sort examples in each batch.\n","                              sort_within_batch=True,\n","                              )\n","\n","# Print number of batches in each split.\n","print('Created `torchtext_train_dataloader` with %d batches!'%len(torchtext_train_dataloader))\n","print('Created `torchtext_valid_dataloader` with %d batches!'%len(torchtext_valid_dataloader))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Created `torchtext_train_dataloader` with 2500 batches!\n","Created `torchtext_valid_dataloader` with 1250 batches!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gHQaFJvYNkKJ"},"source":["### Compare DataLoaders\n","\n","Let's compare the PyTorch DataLoader batches with the PyTorchText BucketIterator batches. We can see how nicely examples of similar length are grouped in same batch with PyTorchText.\n","\n","**Note:** *When using the PyTorchText BucketIterator, make sure to call `create_batches()` before looping through each batch! Else you won't get any output form the iterator.*"]},{"cell_type":"code","metadata":{"id":"hevDHd2uNn3K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628491784176,"user_tz":240,"elapsed":18,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}},"outputId":"2dfd92ce-b6d9-467a-804e-17cf30163e83"},"source":["# Loop through regular dataloader.\n","print('PyTorch DataLoader\\n')\n","for batch in torch_train_dataloader:\n","  \n","  # Let's check batch size.\n","  print('Batch size: %d\\n'% len(batch['text']))\n","  print('LABEL\\tLENGTH\\tTEXT'.ljust(10))\n","\n","  # Print each example.\n","  for text, label in zip(batch['text'], batch['label']):\n","    print('%s\\t%d\\t%s'.ljust(10) % (label, len(text), text))\n","  print('\\n')\n","  \n","  # Only look at first batch. Reuse this code in training models.\n","  break\n","  \n","\n","# Create batches - needs to be called before each loop.\n","torchtext_train_dataloader.create_batches()\n","\n","# Loop through BucketIterator.\n","print('PyTorchText BuketIterator\\n')\n","for batch in torchtext_train_dataloader.batches:\n","\n","  # Let's check batch size.\n","  print('Batch size: %d\\n'% len(batch))\n","  print('LABEL\\tLENGTH\\tTEXT'.ljust(10))\n","  \n","  # Print each example.\n","  for example in batch:\n","    print('%s\\t%d\\t%s'.ljust(10) % (example['label'], len(example['text']), example['text']))\n","  print('\\n')\n","  \n","  # Only look at first batch. Reuse this code in training models.\n","  break"],"execution_count":10,"outputs":[{"output_type":"stream","text":["PyTorch DataLoader\n","\n","Batch size: 10\n","\n","LABEL\tLENGTH\tTEXT\n","pos\t790\tWINCHESTER 73 is the story of a man (Jimmy Stewart) obsessed with getting back his prized possession, a repeating rifle made by Winchester. The rifle keeps changing hands, and Stewart doggedly keeps after it. This 1950 B&W effort by Anthony Mann is more a crime film than a traditional western, and the cowboys often seem more like modern-day gangsters than old-fashioned cowboys. Shelley Winters plays a woman of questionable virtue who is headed for a ranch with a man (Charles Drake) she may marry. She ends up falling for Stewart, but not before she is passed around a bit. Winters is the most complex character in a film filled with unusual characters. Watch for a young Dan Dureyea as a nutty killer and Tony Curtis in a very small role. A woefully miscast Will Geer plays Wyatt Earp.  \n","neg\t1662\tI've been largely convinced to write this review for a number of reasons:<br /><br />1) This is, without doubt, the worst film i've ever seen 2) Unless it gets more reviews it will not be listed in the all time worst films list - which it deserves to be 3) I was kinda lucky - i paid five pound for it. i've seen it in shops for 15 pound. DON NOT PAY THAT MUCH FOR THIS FILM! You will be very angry 4) There are a lot of films out there in the horror genre that are not given a fair rating (in my opinion) and giving this film a higher rating than them is criminal<br /><br />The plot summary: a guy with no friends meets a tramp who promises the world - well, the magic ability to appear to everybody else like somebody else. Our hero cunningly turns into a teenage girl and joins their gang - sitting on swings, baby-sitting. He kills them one by one until he is tracked and found by the police.<br /><br />Why is it so bad? To begin with the acting is VERY VERY bad. Someone else compared it to a school production. No, this is worse than any acting i've seen on a school stage.<br /><br />I've bought a number of these previously banned films from the DVD company vipco and not been as disappointed as i was at this. okay, the acting is bad but the film fails to deliver in every other sense. What was the point in making this film when there isn't even any gore! okay, no gore. What else can a film like this offer? Breats? No, not even any titillation!<br /><br />it's true this film may have a certain charm in its unique naffness but any potential buyer/watcher of this film should be fairly advised that this film is, at best, worth only one out of ten.  \n","neg\t4022\tWhen a friend gave me a boxed set of \"12 Amazing Scifi/Horror Movies!\" I was understandably a little cautious. But, since the item was a gift, I really didn't truly pay my common sense much heed. After all....movies for free! So what if they are a little ropey. After much consideration, Alien Intruder was the first of those movies. Ironically, it was first choice because it looked the best of the bunch. All I can say is, if this is the best of them, I shudder to think what the rest are like.<br /><br />On the surface, it had some good things going for it. Four (count 'em!) actors that I was familiar with. Billy Dee Williams, Tracy Scoggins, Maxwell Caulfield and Jeff Conaway. I told myself...\"Billy and Tracy have been in some good scifi (Star Wars and Babylon 5, respectively) so they wouldn't sign up for a turkey. Max is a veteran soap actor who never really managed to break into film....but not too shoddy an actor. An Jeff....well...he's done the good and the bad as far as films and TV go.\" I was soon to discover that Jeff had decided to add \"the ugly\" to his repertoire of movies.<br /><br />The first clue was in the opening scenes. Jeff mugs his way with gusto through an \"I'm mad\" scene before finally killing himself. An amusing cameo performance, really. Unfortunately this is, without much exaggeration, the highlight of the film. It goes downhill from there.<br /><br />Next up we have the commander of the mission (Williams) who is being sent out to see what happened to Jeff and his crew busy picking his new shipmates from among the ranks of the criminal element. But this assortment aren't so much the Dirty Dozen - more like the Unconvincing Foursome. Plus, one of the crims, a computer hacker, is shown in his cell working away on a laptop computer. Isn't that a bit like letting a murderer run a gun shop in the slammer? Pretty lame prison, if you ask me.<br /><br />When they finally take off the effects are truly horrible. It looks like the spaceship model was knocked up in an afternoon by some bored 8 year old who had parts left over from his Airfix kits.<br /><br />But the horror doesn't stop there. Whilst on route to the area where Jeff's ship vanished, the criminal crew are rewarded for their good behaviour by being given weekends of virtual reality, in which they indulge their male fantasies. All well and good, and the use of scenes from their fantasies serves as an introduction to the \"Alien Menace\" which begins to appear there. But did they have to drag it out for quite sooooo loooooong? Alien Intruder? Alien Boring, more like.<br /><br />Finally they make it to G-Sector and the alien presence makes them fight against each other for her affections until only good old Max is left. The ending, in truly optimistic rubbish film vein, hints at a sequel - as if! Also making an appearance in this movie is a character I'll nickname the \"Sweatdroid\". He's supposed to be an android, but apparently that fact was lost on the make-up crew, who provided him with sweaty features at any opportunity. But don't worry, he's just there to make up the body count numbers at the end.<br /><br />Williams and Scoggins, to be truthful, do very little in the film. They only just barely stay awake, let alone act. And, as I mentioned earlier, Jeff gets an early trip to the showers, so his manicness isn't allowed to enlighten much of the film. Max tries his best, as do a couple of the other cast members, but the movie is just direly atrocious, to be honest.<br /><br />The one, and only, half-way imaginative thing this movie offers is the ship naming convention. They are all named after musicians - Holly, Presley, Joplin. The rest of the film is bland and uninspired.<br /><br />Made in 1992, I had thought, on initial viewing, it was one of those 80's straight-to-video jobs. Looks like they still made crap movies well into the 90's, it seems.<br /><br />It's best avoided. Even as a beer n chips movie this film is a stinker, but at least you can fast forward it, I suppose.  \n","pos\t757\tOK, it was a good American Pie. Erick Stifler goes off to college with his buddy Cooze. During their arrival they meet up with Eric's cousin Dwight. The two pledge to become Betas and along the way they get involved with a whole lot of sex, tits, and some hot girls along the way. In a few words there is a lot more sex, nudity and alcohol. It is a good movie for those who want to enjoy an American Pie movie, granted it isn't as great as the first three is is a good movie. If you enjoy hot girls with really nice tits, get this movie. If you enjoy seeing a bunch of dudes making assholes of themselves, go to this movie. If you want to see the full thing, get the unrated addition. One last thing this is a better attempt than the last two American Pies.  \n","neg\t3423\tForest of the Damned starts out as five young friends, brother & sister Emilio (Richard Cambridge) & Ally (Sophie Holland) along with Judd (Daniel Maclagan), Molly (Nicole Petty) & Andrew (David Hood), set off on a week long holiday 'in the middle of nowhere', their words not mine. Anyway, before they know it they're deep in a forest & Emilio clumsily runs over a woman (Frances Da Costa), along with a badly injured person to add to their problems the van they're travelling in won't start & they can't get any signals on their mobile phones. They need to find help quickly so Molly & Judd wander off in the hope of finding a house, as time goes by & darkness begins to fall it becomes clear that they are not alone & that there is something nasty lurking in the woods...<br /><br />This English production was written & directed by Johannes Roberts & having looked over several other comments & reviews both here on the IMDb & across the internet Forest of the Damned seems to divide opinion with some liking it & other's not, personally it didn't do much for at all. The script is credited on screen to Roberts but here on the IMDb it lists Joseph London with 'additional screenplay material' whatever that means, the film is your basic backwoods slasher type thing like The Texas Chainsaw Massacre (1974) with your basic stranded faceless teenage victims being bumped off but uses the interesting concept of fallen angels who roam the forest & kill people for reason that are never explained to any great deal of satisfaction. Then there's Stephen, played by the ever fantastic Tom Savini, who is never given any sort of justification for what he does. Is he there to get victims for the angels? If so why did he kill Andrew by bashing his head in? The story is very loose, it never felt like a proper film. The character's are poor, the dialogue not much better & the lack of any significant story makes it hard to get into it or care about anything that's going on. Having said that it moves along at a reasonable pace & there are a couple of decent scenes here.<br /><br />Director Johannes doesn't do anything special, it's not a particularly stylish or flash film to look at. There's a few decent horror scenes & the Tom Savini character is great whenever he's on screen (although why didn't he hear Judd breaking the door down with an axe while escaping with Molly?) & it's a shame when he gets killed off. There are a couple of decent gore scenes here, someone has their head bashed in, there's a decapitation, someone gets shotgun blasted, someone throat is bitten out, someones lips are bitten off & someone is ripped in half. There is also a fair amount of full frontal female nudity, not that it helps much.<br /><br />Technically Forest of the Damned is OK, it's reasonably well made but nothing overly special or eye-catching. This was shot in England & Wales & it's quite odd to see an English setting for a very American themed backwards horror. The acting is generally pretty poor save for Savini who deserves to be in better than this. Horror author Shaun Hutson has an embarrassing cameo at the end & proves he should stick to writing rather than acting.<br /><br />Forest of the Damned was a pretty poor horror film, it seems to have fans out there so maybe I'm missing something but it's not a film I have much fondness for. Apart from one or two decent moments there's not much here to recommend.  \n","pos\t908\tWhat a outstanding movie is this i have not words to describe. i don't know how come the rating of the movie is 7.3 it should be 9.3 but anyways no one else can make this movie and the acting by Akshay is just outstanding the second half of the movie makes u cry and the movie has a really good unexpected ending which makes the movie perfect. you should watch this movie once, i think twice well it's up to u. Anyways i love this movie and it's not just sad and funny but it also gives u a really good meanings and what u should be doing so i think this is one of the best movie in the bollywood history. but i know the people has deferent chooses so ya. the only thing i don't like about this movie is the music well the background music is good but the songs are not good enough i think the music would be better if A R Rahman would be the music director but anyways we can't do anything about that so ya.  \n","pos\t2776\tYou know you're in for something different when a movie has Christopher Walken playing the part of a professional hit man - and he isn't even one of the bad guys! Although it could do with some judicious trimming here and there, \"Man on Fire\" is a generally effective crime drama that ranges in tone from the openly sentimental to the downright brutal - and just about every tone imaginable in between.<br /><br />Denzel Washington stars as Creasy, a former CIA assassin who has recently quit the business and is seeking some sort of redemption for the sins he's committed. So far, he's been looking for answers in a bottle and the Bible and not doing all that well with either. As the movie opens, Mexico City has been ravaged by a series of kidnappings aimed at the powerful and well-to-do, possibly perpetrated by the very police force assigned to keep law and order in the community. Creasy accepts the position as bodyguard to the daughter of a wealthy business owner who rightly fears for her safety. The first third of the film is devoted to the growing friendship between Creasy and his charge, Pita, a sweet little girl who, slowly but surely, works her way into Creasy's initially hardened heart and affections. The last two-thirds of the film turns into an Avenging Angel melodrama, as Creasy systematically seeks out and eliminates all those responsible for a tragedy that occurs early on in the story.<br /><br />Based on the novel by A.J Quinnell, \"Man on Fire,\" astutely written by Brian Helgeland and flashily directed by Tony Scott, is a coolly efficient action picture that never shies away from the raw brutality of its subject matter. It takes a risk in asking us to identify with a man who is, for all intents and purposes, achieving his redemption by torturing and murdering (admittedly disreputable) people. These scenes of carnage and violence are both intense and suspenseful, even if they do at times border on the exploitative. Even better are the quiet, intimate moments between Creasy and Pita in the early parts of the movie. Washington and the wonderful Dakota Fanning establish an natural, easygoing rapport that helps to set the stage for the chaos and turmoil to follow.<br /><br />Washington carries the movie with his quality of stoic righteousness, making us understand his character on an emotional level even if what he is doing eludes us intellectually. In addition to the two leads, there are solid performances from Walken, Marc Anthony, Radha Mitchell, Mickey Rourke, Rachel Ticotin and Giancarlo Giannini. But it is Washington and the delightful Ms. Fanning who steal the show.<br /><br />\"Man on Fire\" would have been better with about a half hour taken out its running time, but this is still a better-than-average crime thriller.  \n","pos\t508\tTimeless musical gem, with Gene Kelly in top form, stylish direction by Vincente Minnelli, and wonderful musical numbers. It is great entertainment from start to finish, one of those films that people watch with a smile and say \"they don't make 'em like they used to!\" But they never did quite make them like this. The climactic 25 minute musical sequence without any dialogue is among the most beautiful in film history. Movie magic, clearly derived from the heart and soul of everyone involved. A must see!  \n","pos\t857\tBEWARE SPOILERS. This movie was okay. Goldie Hawn and Chris Sarandon were the best two in it. Okay, so the goofie foreign guy who (SPOILER HERE) trades with the biker for his clothes was funny. This guy's boss was good, too. But the movie really belonged to Sarandon and Hawn. These two should have had a lot more time on screen together. They're chemistry was great. The bathroom scene-WOW! Romantic, sweet, yummy.<br /><br />Hawn is a goofy cocktail waitress who saves a foreign man and ends up at the whitehouse in the middle of a plot due to the greed of politicians. To talk about Sarandon would be to give a lot away. SPOILERS This is a rather untypical romantic/political comedy, and it satisfies both somewhat-the political side a whole lot more than the romantic. It touches on political issues, and just barely skims on romantic areas.<br /><br />  \n","pos\t3315\t\"Home Room\" like \"Zero Day\" and \"Elephant\", was inspired by the recent wave of school shootings. But unlike the other two films, \"Home Room\" focuses on two survivors (not the shooters or those killed) in the aftermath of a shooting. Making it less exploitive and more useful because little effort is wasted in asking questions for which there are no answers.<br /><br />Don't give up on this little film during the first 20 minutes, it is supposed to set up the real story but plays like a rejected \"Hill Street Blues\" episode. It is lame but bear with it, at least it pads the running length enough to get the film classified as a feature. I recommend skipping this entirely and just jumping ahead to the hospital scenes-there is nothing here that you can't pick up from the remainder of the film.<br /><br />Like a lot of good little films this was creatively a one-man show as Paul F. Ryan was both the writer and the director. While this arrangement does not guarantee a good film, it is usually a good sign because it will mean a certain unity of construction and execution that is often lacking in big budget dramatic features. Because the script of \"Home Room\" is its real strength it is fortunate that the writer also executed the production and insured that his vision made it onto the screen.<br /><br />Ryan takes a huge chance with the ending which tests the limits of the average viewer's sentimentality tolerance. He runs it right up to the edge but against all logic leaves you crying instead of cringing. Why the ending works is some combination of the audience need for a reward at the end of this kind of journey, the song (Sarah McLaughlin's \"Sweet Surrender\") he goes out on, and the amazing editing of the final minute.<br /><br />The other strength of the film is the casting of Busy Phillips (Alicia) and Erika Christensen (Deanna) as the main protagonists. Although Phillips plays her standard alienated surly teen and Christensen her intelligent daughter of a good family, they both bring more intensity to their roles than ever before. The family life of both girls is more than satisfactory and of little interest to Ryan. What is happening here is all about the two of them despite a lame side story about a police detective wondering around town trying to tie Alicia to the lone shooter. If they ever re-cut and trim the film this side story should be condensed.<br /><br />A story about two extremely disparate girls bonding and helping each other is hardly a novel idea and Ryan could have easily steered this film into cliché and predictability. But instead his script has them engaging in a fascinating and convincing sparring match, slowly chipping away at each other and sharing moments of vulnerability, only to retreat back inside themselves. Deanna's \"I'm dying inside\" line just tears you apart-I can't think of a moment in any other film that I felt as intensely as that one. She desperately needs a connection that Alicia just as desperately resists. Deanna only makes progress when she retreats. The viewer keeps expecting the group hug that never seems to happen.<br /><br />Ultimately this not only generates a lot of suspense but leaves you admiring both characters and the two actresses who brought them to life.<br /><br />Then again, what do I know? I'm only a child.  \n","\n","\n","PyTorchText BuketIterator\n","\n","Batch size: 10\n","\n","LABEL\tLENGTH\tTEXT\n","pos\t1283\tThe true story of a bunch of junkies robbing a not so honest businessman of drugs, jewelry, guns, and money. Some would say this is the tragic tale of America in the excessive eighties where the high of the peace and free love sixties had crashed into drugs and AIDS. Honestly, this is just regular people with no aim in life who sit around getting high and decide to rob a ruthless man. What is the second part of their master plan? Once they have his stuff...they'll sit around and get high again. Great plan. Even if you don't know the story, there is no suspense in this movie and no surprises. The fact that Cox tries to make some kind of folk heroes out of these characters, with party scenes and a montage of their loot, is weak and insulting. The story was better off with a more straight forward approach. As it is, this is just a sad story of small time drug dealers getting killed by big time drug dealers. The bigger story, in more ways than one, is John Holmes. He is the center of this story anyway. This movie should have been all about him, his life. He was the one in wonderland, with the wonders about to fade away.<br /><br />P.S. Although it isn't official, Boogie Nights is a better version of Holmes life. It isn't entirely factual, but it's far more enjoyable.  \n","pos\t1295\tWhen tradition dictates that an artist must pass his great skills and magic on to an heir, the aging and very proud street performer, known to all as \"The King of Masks,\" becomes desperate for a young man apprentice to adopt and cultivate.<br /><br />His warmth and humanity, tho, find him paying a few dollars for a little person displaced by China's devastating natural disasters, in this case, massive flooding in the 1930's.<br /><br />He takes his new, 7 year old companion, onto his straw houseboat, to live with his prized and beautiful monkey, \"General,\" only to discover that the he-child is a she-child.<br /><br />His life is instantly transformed, as the love he feels for this little slave girl becomes entwined in the stupifying tradition that requires him to pass his art on only to a young man.<br /><br />There are many stories inside this one...many people are touched, and the culture of China opens itself for our Western eye to observe. Thousands of years of heritage boil down into a teacup of drama, and few will leave this DVD behind with a dry eye.<br /><br />The technical transfer itself is not that great, as I found the sound levels all over the meter, and could actually see the video transfer lines in several parts of the movie. Highly recommended :-) 9/10 stars.  \n","neg\t1306\tThis show comes up with interesting locations as fast as the travel channel. It is billed as reality but in actuality it is pure prime time soap opera. It's tries to use exotic locales as a facade to bring people into a phony contest & then proceeds to hook viewers on the contestants soap opera style.<br /><br />It also borrows from an early CBS game show pioneer- Beat The Clock- by inventing situations for its contestants to try & overcome. Then it rewards the winner money. If they can spice it up with a little interaction between the characters, even better. While the game format is in slow motion versus Beat The Clock- the real accomplishment of this series is to escape reality. <br /><br />This show has elements of several types of successful past programs. Reality television, hardly, but if your hooked on the contestants, locale or contest, this is your cup of tea. If your not, this entire series is as I say, drivel dripping with gravy. It is another show hiding behind the reality label which is the trend it started in 2000.<br /><br />It is slick & well produced, so it might last a while yet. After all, so do re-runs of Gilligan's Island, Green Acres, The Beverly Hillbillies & The Brady Bunch. This just doesn't employ professional actors. The intelligence level is about the same.  \n","pos\t1306\tI don't understand your objections to this movie. It is a taut, thrilling extension of the character created in \"Basic Instinct\". The only part of the story that is the least bit unrealistic, is the fact that Sharon Stone's character is still alive and not in jail at this late date.<br /><br />SPOILER ALERT: As the movie progresses, we are presented with three theories of what is going on: 1) Sharon Stone's character is killing all these people because she's crazy (Risk Addicted); 2) David Thewlis' crooked cop is killing these people in order to frame Sharon Stone's character; 3) David Morrissey's analyst is killing these people for revenge. What upsets most people about the movie seems to be that none of these theories are ever explicated as the \"real\" story. (Although the analyst is in a psychiatric care facility for killing the cop; the only killing that occurs on screen.)<br /><br />I think this is a brilliant plot device in the spirit of \"2001, A Space Odyssey.\" WHO CARES what is real? The blonde really is crazy, the cop really is crooked and the analyst really wants revenge. What's important is the interactions between these and other characters in the story. Like real life, everyone is more complicated than anyone thinks and reality is more complicated than a movie. Get over it!  \n","pos\t1309\tWill Smith delivers yet again in a film about a man with the weight of the world on his shoulders and his crusade to right his wrongs in a way that will touch even the most hardened of hearts!!! Writer Grant Nieporte and Italian Director Gabriele Muccino come together and created a masterpiece that I highly recommend to purchase and keep in your movie collection as you will never grow tired of watching/feeling this film!!! I have the Highest Respects for Will Smith as he is not only a brilliant Actor but one can tell he has a genuine love for people and life which no doubt made him perfect for the character (IRS Agent Ben Thomas) he played in this film. You will find yourself feeling his pain and anger, the frustrations over his love for Emily, played by Rosario Dawson, who by the way was Fantastic as usual. I found myself falling in love with the fact their characters were falling in love. Woody Harrelson also stars in this Top Notch film. I find it very difficult to write this review without giving away key plot points...All I can say is, Watch it and when you do make sure you have nothing to interrupt you, take the phone off the hook, sit back and get ready to start trying to unravel the mysterious life and past of IRS Agent Ben Thomas...I thank you Will Smith for another Great Film!!!  \n","pos\t1315\tTiempo de Valientes fits snugly into the buddy action movie genre, but transcends its roots thanks to excellent casting, tremendous rapport between its leads, and outstanding photography. Diego Peretti stars as Dr. Silverstein, a shrink assigned to ride shotgun with detective Diaz (Luis Luque), who's been assigned to investigate the murder of two minor hoods who seem to have been involved in am arms smuggling conspiracy. Diaz has been suspended from duty, but he's the best man for the job and must have professional psychiatric help in order to be reinstated. Silverstein and Diaz soon find themselves enmeshed in a conspiracy involving Argentina's intelligence community and some uranium, and the film separates them at a crucial point that allows Silverstein to develop some impressive sleuthing skills of his own. Peretti and Luque are excellent together and remind me of screen team Terence Hill and Bud Spencer, though Peretti isn't as classically handsome as Hill. Remarkably, even at almost two hours in length Tiempo de Valientes doesn't wear out its welcome, and indeed writer-director Damian Szifron sets up a potential sequel in the film's charming coda. All in all, a wonderful and very entertaining action comedy that neither panders to the lowest common denominator nor insults your intelligence.  \n","neg\t1315\tIt just seems bizarre that someone read this script, and thought, \"This is funny! I mean, it's so hilarious it just has to be made!\" Who was this person? Is he or she the person really responsible for this? Are they the one's who owe me for my time, more so than the director/writer?<br /><br />This film stinks in most every way possible. There's no one shred of good dialogue, and not one likable character. And the story...<br /><br />I prefer the 2nd worst movie ever, Hulk Hogan's \"No Hold's Barred\" to this by quite a considerable degree. It seems almost Shakespearen in comparison.<br /><br />The ending is padded out with several minutes of outtakes, and it's still under 80 minutes. The outtakes include cast members laughing at the 'hilarious' mistakes they've made, and things that went wrong on the set of this 'comedy.' Glad to see someone laughing in someway, with some connection to this 'film.'<br /><br />Nothing in this film is funny. Nothing. It just goes on, and on. It's truly that lame. I love films that are so bad they're good. This is so bad it's...something, but I don't know what, and hopefully will never find out.<br /><br />Amanda Peet doesn't suck outright, and is in fact the only half good thing about this wannabe film. But, that really means little.<br /><br />Avoid at all costs.  \n","neg\t1316\tSTAR RATING: ***** The Works **** Just Misses the Mark *** That Little Bit In Between ** Lagging Behind * The Pits <br /><br />In this debut effort for Nick Park's beloved man and dog, they are forced to fly to the moon when good old Wallace runs out of cheese.<br /><br />As well as being the shortest feature at just 22 minutes, this W/G adventure is also the earliest and it kinda shows. The plasticine animation is a little creaky and funny here, sort of reminiscent of the Mork animation about the little man in the box.<br /><br />Admirable though the craftsmanship behind it is, I've never actually been hugely into Wallace & Gromit (maybe a bit too clean and traditional for someone of my generation.) The only one I've really enjoyed is The Wrong Trousers (and that was more from when I was younger and less aware of, shall we say, the seedier pleasures of life.) I was driven to actively seek out this early effort due to the resurgence in popularity as a result of the hugely successful recent film adaptation.<br /><br />As technically impressive as the first two (all things considered!) this one lacks the emotional angle it's successors were to possess. That being said, it's fairly good fun as a first try and certainly set the standard for greater things to come. Two stars, but a good two stars. **  \n","pos\t1317\tI want Céline and Jessie go further in their relationship, I want to tell them that they were made for each other, that in a lot of moment in the film we want they to die for each other. Their story is what we ever wanted and probably most of us never reached. This is about love but not stupid things like in \"notting hills\" or those kind of movie. This is life and i did believe in them, i did believe they were falling... This was so clever and touching. I have just finished to view it a minute ago and i m still there... I want to go to Vienna. I want to see them as soon as possible again.<br /><br />I have to say i was now becoming misanthropist and felt like if love was just a fake, a concept, but with this movie i realized that maybe somewhere, somehow and some when, something could really happen.<br /><br />I'm french and didn't know very well July Delpy despite Kieslowski \"three colors : white\"... Now i have to see her other works because she looks like an angel and got a perfect acting.<br /><br />i saw \"before sunset\" (the sequel in Paris) a few days before i saw \"before sunrise\" and their is no matter. They are both masterpieces. proof that you don't need to impress the eyes with technology to get pure feelings. I'm sorry for my English which i m trying to best.<br /><br />Franck in France  \n","pos\t1326\tHaving seen and loved Greg Lombardo's most recent film \"Knots\" (he co-wrote and directed that feature as well), I decided to check out his earlier work, and this movie was well worth the effort and rental. Macbeth in Manhattan is a tongue in cheek, excellent take on the Shakespeare favorite, updated and moved to NYC. I was impressed by the underlying wit and intelligence of the script and was wowed by the way the storyline of the production in the movie mirrors the storyline of the play itself - and very cleverly at that. The trials and tribulations of life in Manhattan parallel many a Shakespeare play, and Central Park was rarely put to better use than as the woods around Macbeth's castle. Mr. Lombardo obviously has a fond place in his heart for New York and New York stories (Knots is a funny and warm sex comedy about six thirty-something New Yorkers set primarily in a charming Brooklyn neighborhood, with Manhattan offices and a downtown loft thrown in for good measure) and has spent considerable time around the plays of Shakespeare. The movie is well-paced and the story reflects a deep understanding of the essential drama at the core of Macbeth. It reminded me of Al Pacino's \"Looking for Richard\" - another wonderful Shakespeare \"play within a movie.\" I highly recommend checking out Macbeth in Manhattan.  \n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2IEb5wL9Qudr"},"source":["### Train Loop Examples\n","\n","Now let's look at a model training loop would look like. I printed the first 10 batches list of examples lengths to show how nicely they are grouped throughout the dataset!"]},{"cell_type":"code","metadata":{"id":"5oCIpdJCQwqZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628491784177,"user_tz":240,"elapsed":17,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}},"outputId":"5d9c9099-9d8b-4a01-c255-ee2f4c0e3d47"},"source":["# Example of number of epochs\n","epochs = 1\n","\n","# Example of loop through each epoch\n","for epoch in range(epochs):\n","\n","  # Create batches - needs to be called before each loop.\n","  torchtext_train_dataloader.create_batches()\n","\n","  # Loop through BucketIterator.\n","  for sample_id, batch in enumerate(torchtext_train_dataloader.batches):\n","    print('Batch examples lengths: %s'.ljust(20) % str([len(example['text']) for example in batch]))\n","\n","    # Let's break early, you get the idea.\n","    if sample_id == 10:\n","      break"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Batch examples lengths: [596, 598, 600, 601, 602, 602, 605, 609, 609, 610]\n","Batch examples lengths: [2205, 2209, 2213, 2215, 2218, 2233, 2267, 2274, 2274, 2284]\n","Batch examples lengths: [1141, 1142, 1143, 1143, 1144, 1145, 1155, 1156, 1157, 1158]\n","Batch examples lengths: [1107, 1108, 1109, 1109, 1111, 1112, 1114, 1119, 1121, 1123]\n","Batch examples lengths: [613, 614, 618, 619, 619, 622, 622, 624, 624, 629]\n","Batch examples lengths: [1433, 1436, 1437, 1440, 1447, 1454, 1460, 1465, 1468, 1469]\n","Batch examples lengths: [3230, 3241, 3300, 3310, 3316, 3326, 3327, 3348, 3356, 3371]\n","Batch examples lengths: [677, 677, 677, 678, 679, 680, 681, 682, 683, 683]\n","Batch examples lengths: [1472, 1475, 1476, 1476, 1486, 1492, 1492, 1495, 1498, 1510]\n","Batch examples lengths: [747, 748, 752, 755, 755, 755, 756, 757, 758, 759]\n","Batch examples lengths: [2055, 2071, 2079, 2080, 2095, 2104, 2111, 2117, 2119, 2123]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"POrx0_EuImpM"},"source":["## Using PyTorchText TabularDataset\n","\n","Now I will use the TabularDataset functionality which creates the PyTorchDataset object right from our local files. \n","\n","We don't need to create a custom PyTorch Dataset class to load our dataset as long as we have tabular files of our data."]},{"cell_type":"markdown","metadata":{"id":"SGNgKXkq5X8f"},"source":["### Data to Files\n","\n","Since our dataset is scattered into multiple files, I created a function `files_to_tsv` which puts our dataset into a `.tsv` file (Tab-Separated Values).\n","\n","Since I'll use the **TabularDataset** from `pytorch.data` I need to pass tabular format files.\n","\n","For text data I find the Tab Separated Values format easier to deal with.\n","\n","I will call the `files_to_tsv` function for each of the two partitions **train** and **test**. \n","\n","The function will return the name of the `.tsv` file saved so we can use it later in PyTorchText."]},{"cell_type":"code","metadata":{"id":"sn8cBvTx0NLr","colab":{"base_uri":"https://localhost:8080/","height":251,"referenced_widgets":["0540dbfd2b304fb5a4338ac177d5c1e6","583d1b6545d642eb8be424dcc8dde698","17f566fc253c4ebeb031f84f71760d3b","b4c11650b68540b1bdb2e1ce811e25aa","3c99d7ff2b0a4ca2b3c9923e840a0dae","84f8753830a642059ebdf22dbba770e5","2c31069f96754d91ae60faf04dbe737a","65fa71e70c4241808d2d406dcb309ef4","8216a3f8111242758da8e39931526df7","080c947f50b4423e9335c989752cedd1","848eef63256640b6947acb19030ab659","97768141e7254083a7899cec5a3ba2c4","dbb67e057fd74b83930b328777b9643b","78839ad5766c4f398495979c878635cf","6f186dbfb3cc4c88b7e57f5d1c6abe49","c87d6401cf7c4eec8c39e59e8f87aeb8","a64f5eb6b7114c4e8ca02f0c3f54b87a","d6c09a02382a41ef894e50555bcd8312","38a0772c85ef472c9a41aa1e8fd44d56","7c64071a23934f86803ca12aa370a667","8a4a994e84e649b5800dcb46b9a47098","c2513e3d1a1140298a3b81c6d60078d7","608f10bfedb9464fa81bb47aaf43f4d7","1c0a648846434bf1969869a2978ba253","bfbcdddac955436d8cc11cbd99aa520f","068bdeb7efe74371802b4f347ae6e214","4eba572c10c04818a967f00f491ed36e","9608b6f4cc8a41d2be2f1f4f854e1221","ffb22841789c4550ad1e85665ec5ad4e","2243d9f70a0a4fe1a1a19b65b8872506","d0f850e4dcab47ff9067e1f7724aeecb","ec501c5cbec6414cb8244ab8ff198ad4"]},"executionInfo":{"status":"ok","timestamp":1628491798728,"user_tz":240,"elapsed":14559,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}},"outputId":"6d421304-415c-479b-ae9e-c13be945dd49"},"source":["def files_to_tsv(partition_path, save_path='./'):\n","  \"\"\"Parse each file in partition and keep track of sentiments.\n","  Create a list of pairs [tag, text]\n","\n","  Arguments:\n","\n","    partition_path (:obj:`str`):\n","      Partition used: train or test.\n","\n","    save_path (:obj:`str`):\n","      Path where to save the final .tsv file.\n","\n","  Returns:\n","\n","    :obj:`str`: Filename of created .tsv file.\n","\n","  \"\"\"\n","\n","  # List of all examples in format [tag, text].\n","  examples = []\n","\n","  # Print partition.\n","  print(partition_path)\n","\n","  # Loop through each sentiment.\n","  for sentiment in ['pos', 'neg']:\n","\n","    # Find path for sentiment.\n","    sentiment_path = os.path.join(partition_path, sentiment)\n","\n","    # Get all files from path sentiment.\n","    files_names = os.listdir(sentiment_path)\n","\n","    # For each file in path sentiment.\n","    for file_name in tqdm(files_names, desc=f'{sentiment} Files'):\n","\n","      # Get file content.\n","      file_content = io.open(os.path.join(sentiment_path, file_name), mode='r', encoding='utf-8').read()\n","\n","      # Fix any format errors.\n","      file_content = fix_text(file_content)\n","\n","      # Append sentiment and file content.\n","      examples.append([sentiment, file_content])\n","\n","  # Create a TSV file with same format `sentiment  text`.\n","  examples = [\"%s\\t%s\"%(example[0], example[1]) for example in examples]\n","\n","  # Create file name.\n","  tsv_filename = os.path.basename(partition_path) + '_pos_neg_%d.tsv'%len(examples)\n","\n","  # Write to TSV file.\n","  io.open(os.path.join(save_path, tsv_filename), mode='w', encoding='utf-8').write('\\n'.join(examples))\n","\n","  # Return TSV file name.\n","  return tsv_filename\n","  \n","\n","# Path where to save tsv file.\n","data_path = '/content'\n","\n","# Convert train files to tsv file.\n","train_filename = files_to_tsv(partition_path='/content/aclImdb/train', save_path=data_path)\n","\n","# Convert test files to tsv file.\n","test_filename = files_to_tsv(partition_path='/content/aclImdb/test', save_path=data_path)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/content/aclImdb/train\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0540dbfd2b304fb5a4338ac177d5c1e6","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='pos Files', max=12500.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8216a3f8111242758da8e39931526df7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='neg Files', max=12500.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","/content/aclImdb/test\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a64f5eb6b7114c4e8ca02f0c3f54b87a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='pos Files', max=12500.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bfbcdddac955436d8cc11cbd99aa520f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='neg Files', max=12500.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l8rfZyyp4tGR"},"source":["### TabularDataset\n","\n","Here I setup the data fields for PyTorchText. We have to tell the library how to handle each column of the `.tsv` file. For this we need to create `data.Field` objects for each column.\n","\n","`text_tokenizer`: \n","For this example I don't use an actual tokenizer for the `text` column but I need to create one because it requires as input. I created a dummy tokenizer that returns same value. Depending on the project, here is where you will have your own tokenizer. It needs to take as input text and output a list.\n","\n","`label_tokenizer`\n","The label tokenizer is also a dummy tokenizer. This is where you will have a encoder to transform labels to ids.\n","\n","Since we have two `.tsv` files it's great that we can use the `.split` function from **TabularDataset** to handle two files at the same time one for train and the other one for test.\n","\n","Find more details about **torchtext.data** functionality [here](https://torchtext.readthedocs.io/en/latest/data.html#dataset-batch-and-example)."]},{"cell_type":"code","metadata":{"id":"dhaX2V5N3Y4V","executionInfo":{"status":"ok","timestamp":1628491967626,"user_tz":240,"elapsed":1407,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}}},"source":["# Text tokenizer function - dummy tokenizer to return same text.\n","# Here you will use your own tokenizer.\n","text_tokenizer = lambda x : x\n","\n","# Label tokenizer - dummy label encoder that returns same label.\n","# Here you will add your own label encoder.\n","label_tokenizer = lambda x: x\n","\n","# Data field for text column - invoke tokenizer.\n","TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=text_tokenizer, lower=False)\n","\n","# Data field for labels - invoke tokenize label encoder.\n","LABEL = torchtext.legacy.data.Field(sequential=True, tokenize=label_tokenizer, use_vocab=False)\n","\n","# Create data fields as tuples of description variable and data field.\n","datafields = [(\"label\", LABEL),\n","              (\"text\", TEXT)]\n","\n","# Since we have have tab separated data we use TabularDataset\n","train_dataset, valid_dataset = torchtext.legacy.data.TabularDataset.splits(\n","    \n","                                                # Path to train and validation.\n","                                                path=data_path,\n","\n","                                                # Train data filename.\n","                                                train=train_filename,\n","\n","                                                # Validation file name.\n","                                                validation=test_filename,\n","\n","                                                # Format of local files.\n","                                                format='tsv',\n","\n","                                                # Check if we have header.\n","                                                skip_header=False,\n","\n","                                                # How to handle fields.\n","                                                fields=datafields)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fsmI67Ly4zkc"},"source":["### PyTorchText Bucket Iterator Dataloader\n","\n","I'm using same setup as in the **PyTorchText Bucket Iterator Dataloader** code cell section. The only difference is in the `sort_key` since there is different way to access example attributes (we had dictionary format before)."]},{"cell_type":"code","metadata":{"id":"50Uno-i7Pwor","executionInfo":{"status":"aborted","timestamp":1628491799348,"user_tz":240,"elapsed":6,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}}},"source":["# Group similar length text sequences together in batches.\n","torchtext_train_dataloader, torchtext_valid_dataloader = torchtext.data.BucketIterator.splits(\n","    \n","                              # Datasets for iterator to draw data from\n","                              (train_dataset, valid_dataset),\n","\n","                              # Tuple of train and validation batch sizes.\n","                              batch_sizes=(train_batch_size, valid_batch_size),\n","\n","                              # Device to load batches on.\n","                              device=device, \n","\n","                              # Function to use for sorting examples.\n","                              sort_key=lambda x: len(x.text),\n","\n","\n","                              # Repeat the iterator for multiple epochs.\n","                              repeat=True, \n","\n","                              # Sort all examples in data using `sort_key`.\n","                              sort=False, \n","\n","                              # Shuffle data on each epoch run.\n","                              shuffle=True,\n","\n","                              # Use `sort_key` to sort examples in each batch.\n","                              sort_within_batch=True,\n","                              )\n","\n","# Print number of batches in each split.\n","print('Created `torchtext_train_dataloader` with %d batches!'%len(torchtext_train_dataloader))\n","print('Created `torchtext_valid_dataloader` with %d batches!'%len(torchtext_valid_dataloader))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h2bJtSlW42Fo"},"source":["### Compare DataLoaders\n","\n","Let's compare the PyTorch DataLoader batches with the PyTorchText BucketIterator batches created with TabularDataset. We can see how nicely examples of similar length are grouped in same batch with PyTorchText.\n","\n","**Note:** *When using the PyTorchText BucketIterator, make sure to call `create_batches()` before looping through each batch! Else you won't get any output form the iterator.*"]},{"cell_type":"code","metadata":{"id":"U7oII00Xz7e5","executionInfo":{"status":"aborted","timestamp":1628491799348,"user_tz":240,"elapsed":5,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}}},"source":["# Loop through regular dataloader.\n","print('PyTorch DataLoader\\n')\n","for batch in torch_train_dataloader:\n","  \n","  # Let's check batch size.\n","  print('Batch size: %d\\n'% len(batch['text']))\n","  print('LABEL\\tLENGTH\\tTEXT'.ljust(10))\n","\n","  # Print each example.\n","  for text, label in zip(batch['text'], batch['label']):\n","    print('%s\\t%d\\t%s'.ljust(10) % (label, len(text), text))\n","  print('\\n')\n","  \n","  # Only look at first batch. Reuse this code in training models.\n","  break\n","  \n","\n","# Create batches - needs to be called before each loop.\n","torchtext_train_dataloader.create_batches()\n","\n","# Loop through BucketIterator.\n","print('PyTorchText BuketIterator\\n')\n","for batch in torchtext_train_dataloader.batches:\n","\n","  # Let's check batch size.\n","  print('Batch size: %d\\n'% len(batch))\n","  print('LABEL\\tLENGTH\\tTEXT'.ljust(10))\n","  \n","  # Print each example.\n","  for example in batch:\n","    print('%s\\t%d\\t%s'.ljust(10) % (example.label, len(example.text), example.text))\n","  print('\\n')\n","  \n","  # Only look at first batch. Reuse this code in training models.\n","  break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d2QsZ5yN8iDi"},"source":["### Train Loop Examples\n","\n","Now let's look at a model training loop would look like. I printed the first 10 batches list of examples lengths to show how nicely they are grouped throughout the dataset!\n","\n","We see that we get same exact behavior as we did when using PyTorch Dataset. Now it depends on which way is easier for you to use PyTorchText BucketIterator: with PyTorch Dataset or with PyTorchText TabularDataset"]},{"cell_type":"code","metadata":{"id":"M9WOC2Xe0MQs","executionInfo":{"status":"aborted","timestamp":1628491799349,"user_tz":240,"elapsed":6,"user":{"displayName":"Carlos Walter Pacheco Lora","photoUrl":"","userId":"05889892519883337793"}}},"source":["# Example of number of epochs.\n","epochs = 1\n","\n","# Example of loop through each epoch.\n","for epoch in range(epochs):\n","\n","  # Create batches - needs to be called before each loop.\n","  torchtext_train_dataloader.create_batches()\n","\n","  # Loop through BucketIterator.\n","  for sample_id, batch in enumerate(torchtext_train_dataloader.batches):\n","    # Put all example.text of batch in single array.\n","    batch_text = [example.text for example in batch]\n","\n","    print('Batch examples lengths: %s'.ljust(20) % str([len(text) for text in batch_text]))\n","\n","    # Let's break early, you get the idea.\n","    if sample_id == 10:\n","      break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1YnDV4IQWF2v"},"source":["## **Final Note**\n","\n","If you made it this far **Congrats!** 🎊 and **Thank you!** 🙏 for your interest in my tutorial!\n","\n","I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow.\n","\n","Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials!\n","\n","If you see something wrong please let me know by opening an issue on my [ml_things GitHub repository](https://github.com/gmihaila/ml_things/issues)!\n","\n","A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.\n","\n","## **Contact** 🎣\n","\n","🦊 GitHub: [gmihaila](https://github.com/gmihaila)\n","\n","🌐 Website: [gmihaila.github.io](https://gmihaila.github.io/)\n","\n","👔 LinkedIn: [mihailageorge](https://medium.com/r/?url=https%3A%2F%2Fwww.linkedin.com%2Fin%2Fmihailageorge)\n","\n","📬 Email: [georgemihaila@my.unt.edu.com](mailto:georgemihaila@my.unt.edu.com?subject=GitHub%20Website)"]}]}