{"cells":[{"cell_type":"markdown","metadata":{"id":"Yg90deR13qYE"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/040_encoder_decoder/encoder_decoder.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"4Kzp2jHl3qY7"},"source":["# La arquitectura *Encoder-Decoder*"]},{"cell_type":"markdown","metadata":{"id":"pf2egeh0C_ER"},"source":["En posts anteriores hemos visto como podemos utilizar `redes neuronales recurrentes` para [generaci√≥n de texto](https://sensioai.com/blog/037_charRNN) as√≠ como [clasificaci√≥n de texto](https://sensioai.com/blog/038_clasificacion_texto). En ambas aplicaciones hemos entrenado una red neuronal que alimentamos con una secuencia de texto, ya sean letras o palabras en una frase, a la cual le pedimos a la salida una distribuci√≥n de probabilidad sobre la diferentes categor√≠as (para el caso de la clasificaci√≥n) o directamente el vocabulario (para la generaci√≥n de texto). La principal limitaci√≥n de estos modelos es que no podemos obtener m√°s que una salida, y es por esto que en el caso de la generaci√≥n de texto concatenamos la salida en cada instante a las entradas para utilizarlo de nuevo como entradas y obtener as√≠ una nueva predicci√≥n. En este post vamos a ver c√≥mo podemos implementar modelos que no s√≥lo sean capaces de recibir secuencias a la entrada, sino que tambi√©n puedan dar secuencias de longitud arbitraria a la salida. Este tipo de modelos se conocen como modelos *sequence to sequence* (o simplemente *seq2seq*) y pueden ser utilizados para tareas tales como la generaci√≥n de texto, traducci√≥n entre idiomas, resumir textos, etc.\n","\n","![](https://pytorch.org/tutorials/_images/seq2seq.png)"]},{"cell_type":"markdown","metadata":{"id":"bd8YnvbvC_ER"},"source":["> üí° Este post est√° basado en el siguiente [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), en el que podr√°s encontrar m√°s informaci√≥n."]},{"cell_type":"markdown","metadata":{"id":"BtOkvrdIC_ES"},"source":["## El *dataset*"]},{"cell_type":"markdown","metadata":{"id":"zma1-yBoC_ES"},"source":["En este post vamos a ver c√≥mo entrenar este tipo de arquitectura para traducir texto del ingl√©s al castellano. Puedes encontrar un ejemplo de dataset para traducci√≥n [aqu√≠](https://www.manythings.org/anki/). Una vez descargados los datos vamos a leer el archivo, separando los pares de frases de cada ejemplo."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2378,"status":"ok","timestamp":1686932132675,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"CtzDFtguEHMK","outputId":"3953708b-081c-4867-8ecc-068ec673446e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1686932132676,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"Rd6sDB6RC_ET"},"outputs":[],"source":["import unicodedata\n","import re\n","\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    return s\n","\n","def read_file(file, reverse=False):\n","    # Read the file and split into lines\n","    lines = open(file, encoding='utf-8').read().strip().split('\\n')\n","\n","    # Split every line into pairs and normalize\n","    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n","\n","    return pairs"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7261,"status":"ok","timestamp":1686932139934,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"6RinX_1uC_EU"},"outputs":[],"source":["pairs = read_file('/content/drive/MyDrive/Colab Notebooks/deep_learning/18 nlp/04 traduccion/spa.txt')\n","\n","# pairs = read_file('spa.txt')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1686932139934,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"uHjkl39BC_EV","outputId":"64450a75-54d3-49de-c0fe-b33b3f457935"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['he was reading a newspaper .', 'el estaba leyendo el periodico .']"]},"metadata":{},"execution_count":4}],"source":["import random\n","\n","random.choice(pairs)"]},{"cell_type":"markdown","metadata":{"id":"IxhmrK8tC_EW"},"source":["Como ya hemos visto en los posts anteriores, necesitamos un `tokenizer`. En este caso, la clase `Lang` se encargar√° de asignar un √≠ndice √∫nico a cada palabra calculando tambi√©n su frecuencia para, m√°s tarde, poder quedarnos s√≥lo con las palabras m√°s frecuentes. Necesitaremos, adem√°s, dos nuevos *tokens* especiales: el token `<eos>` y el token `<sos>` para indicar, respectivamente, el inicio y final de una frase. M√°s adelante veremos c√≥mo utilizarlos."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686932139935,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"OhvIdvs0C_EX"},"outputs":[],"source":["SOS_token = 0\n","EOS_token = 1\n","PAD_token = 2\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {\"SOS\": 0, \"EOS\": 1, \"PAD\": 2}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n","        self.n_words = 3  # Count SOS, EOS and PAD\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","    def indexesFromSentence(self, sentence):\n","        return [self.word2index[word] for word in sentence.split(' ')]\n","\n","    def sentenceFromIndex(self, index):\n","        return [self.index2word[ix] for ix in index]"]},{"cell_type":"markdown","metadata":{"id":"YefvAE19C_EX"},"source":["Opcionalmente, tambi√©n podemos indicar la longitud m√°xima de las frases a utilizar as√≠ como un conjunto de comienzos de frases que queramos filtrar. Esto lo hacemos √∫nicamente para acelerar el proceso de entrenamiento, trabajando con un conjunto peque√±o de datos."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686932139935,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"moCeuAwPC_EY"},"outputs":[],"source":["MAX_LENGTH = 10\n","\n","eng_prefixes = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \"\n",")\n","\n","\n","def filterPair(p, lang, filters, max_length):\n","    return len(p[0].split(' ')) < max_length and \\\n","        len(p[1].split(' ')) < max_length and \\\n","        p[lang].startswith(filters)\n","\n","def filterPairs(pairs, filters, max_length, lang=0):\n","    return [pair for pair in pairs if filterPair(pair, lang, filters, max_length)]"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6723,"status":"ok","timestamp":1686932146655,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"g90yDZq9C_EY","outputId":"704dafe4-0744-4cbe-b628-3cc4dd27945f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tenemos 134142 pares de frases\n","Longitud vocabularios:\n","spa 13552\n","eng 26182\n"]},{"output_type":"execute_result","data":{"text/plain":["['a promise is a promise . EOS', 'una promesa es una promesa . EOS']"]},"metadata":{},"execution_count":7}],"source":["def prepareData(file, filters=None, max_length=None, reverse=False):\n","\n","    pairs = read_file(file, reverse)\n","    print(f\"Tenemos {len(pairs)} pares de frases\")\n","\n","    if filters is not None:\n","        assert max_length is not None\n","        pairs = filterPairs(pairs, filters, max_length, int(reverse))\n","        print(f\"Filtramos a {len(pairs)} pares de frases\")\n","\n","    # Reverse pairs, make Lang instances\n","    if reverse:\n","        pairs = [list(reversed(p)) for p in pairs]\n","        input_lang = Lang('eng')\n","        output_lang = Lang('spa')\n","    else:\n","        input_lang = Lang('spa')\n","        output_lang = Lang('eng')\n","\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","\n","        # add <eos> token\n","        pair[0] += \" EOS\"\n","        pair[1] += \" EOS\"\n","\n","    print(\"Longitud vocabularios:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","\n","    return input_lang, output_lang, pairs\n","\n","input_lang, output_lang, pairs = prepareData('/content/drive/MyDrive/Colab Notebooks/deep_learning/18 nlp/04 traduccion/spa.txt')\n","\n","# descomentar para usar el dataset filtrado\n","#input_lang, output_lang, pairs = prepareData('spa.txt', filters=eng_prefixes, max_length=MAX_LENGTH)\n","\n","random.choice(pairs)"]},{"cell_type":"markdown","metadata":{"id":"uF8Zow3nC_EY"},"source":["Una vez construidos los dos vocabularios, podemos obtener los √≠ndices a partir de una frase, y viceversa, de la siguiente manera."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1686932146656,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"IXWGi2GkC_EZ","outputId":"cfbc2abb-628f-4803-c075-09ad8e6c31f5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[72, 5394, 143, 4]"]},"metadata":{},"execution_count":8}],"source":["output_lang.indexesFromSentence('tengo mucha sed .')"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1686932146656,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"5P2QM37dC_EZ","outputId":"7fbd4a11-75d0-478d-bf5e-f94fc63fba70"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['tengo', 'mucha', 'sed', '.']"]},"metadata":{},"execution_count":9}],"source":["output_lang.sentenceFromIndex([72, 5394, 143, 4])"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686932146656,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"XvxZM994yBhc"},"outputs":[],"source":["# input_lang.sentenceFromIndex([72, 5394, 143, 4])\n","# input_lang.indexesFromSentence('i have much money .')"]},{"cell_type":"markdown","metadata":{"id":"cFmdmdQpC_EZ"},"source":["Para terminar, las siguientes clases se encargar√°n de alimentar nuestro modelo *seq2seq* utilizando las clases `Dataset` y `DataLoader` de `Pytorch`. Debido a que nuestras frases pueden tener diferentes longitudes, tenemos que asegurarnos que al construir nuestros batches todas tengan la misma longitud, ya que para alimentar la red necesitamos tensores rectangulares. Esto lo conseguimos con la funci√≥n `collate`."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1342,"status":"ok","timestamp":1686932147995,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"qHqk7a_jC_EZ","outputId":"2cb54697-a64e-43ed-b55e-5ae9eba2182e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(107313, 26829)"]},"metadata":{},"execution_count":11}],"source":["import torch\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, input_lang, output_lang, pairs):\n","        self.input_lang = input_lang\n","        self.output_lang = output_lang\n","        self.pairs = pairs\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, ix):\n","        return torch.tensor(self.input_lang.indexesFromSentence(self.pairs[ix][0]), device=device, dtype=torch.long), \\\n","               torch.tensor(self.output_lang.indexesFromSentence(self.pairs[ix][1]), device=device, dtype=torch.long)\n","\n","    def collate(self, batch):\n","        # calcular longitud m√°xima en el batch\n","        max_input_len, max_output_len = 0, 0\n","        for input_sentence, output_sentence in batch:\n","            max_input_len = len(input_sentence) if len(input_sentence) > max_input_len else max_input_len\n","            max_output_len = len(output_sentence) if len(output_sentence) > max_output_len else max_output_len\n","        # a√±adimos padding a las frases cortas para que todas tengan la misma longitud\n","        input_sentences, output_sentences = [], []\n","        for input_sentence, output_sentence in batch:\n","            input_sentences.append(torch.nn.functional.pad(input_sentence, (0, max_input_len - len(input_sentence)), 'constant', self.input_lang.word2index['PAD']))\n","            output_sentences.append(torch.nn.functional.pad(output_sentence, (0, max_output_len - len(output_sentence)), 'constant', self.output_lang.word2index['PAD']))\n","        # opcionalmente, podr√≠amos re-ordenar las frases en el batch (algunos modelos lo requieren)\n","        return torch.stack(input_sentences), torch.stack(output_sentences)\n","\n","# separamos datos en train-test\n","train_size = len(pairs) * 80 // 100\n","train = pairs[:train_size]\n","test = pairs[train_size:]\n","\n","dataset = {\n","    'train': Dataset(input_lang, output_lang, train),\n","    'test': Dataset(input_lang, output_lang, test)\n","}\n","\n","len(dataset['train']), len(dataset['test'])"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1793,"status":"ok","timestamp":1686932149785,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"RDktEuzUC_Ea","outputId":"21f9c696-6b64-45ba-cee4-eb5d3ec90abc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([3, 4, 1], device='cuda:0'), tensor([5, 4, 1], device='cuda:0'))"]},"metadata":{},"execution_count":12}],"source":["input_sentence, output_sentence = dataset['train'][1]\n","\n","input_sentence, output_sentence"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1686932149786,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"cBI5YUupC_Ea","outputId":"b668c85b-fe67-48ee-f0c0-b8ad87a3bad1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['go', '.', 'EOS'], ['vete', '.', 'EOS'])"]},"metadata":{},"execution_count":13}],"source":["input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1686932149787,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"J58o8kWVC_Ea","outputId":"e766b45c-ef93-4c53-9fce-c81669a5a61a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([64, 12]), torch.Size([64, 12]))"]},"metadata":{},"execution_count":14}],"source":["dataloader = {\n","    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=64, shuffle=True, collate_fn=dataset['train'].collate),\n","    'test': torch.utils.data.DataLoader(dataset['test'], batch_size=256, shuffle=False, collate_fn=dataset['test'].collate),\n","}\n","\n","inputs, outputs = next(iter(dataloader['train']))\n","inputs.shape, outputs.shape"]},{"cell_type":"markdown","metadata":{"id":"ofqSU7vVC_Eb"},"source":["## El modelo"]},{"cell_type":"markdown","metadata":{"id":"1PHHx68cC_Eb"},"source":["Una vez tenemos nuestros *dataloaders* listos, vamos a ver c√≥mo construir nuestro modelo siguiendo la arquitectura `encoder-decoder`."]},{"cell_type":"markdown","metadata":{"id":"WXV1uwhtC_Eb"},"source":["### El *encoder*"]},{"cell_type":"markdown","metadata":{"id":"RXTvNW4OC_Eb"},"source":["Como `encoder` utilizaremos una `red neuronal recurrente` como las que ya hemos utilizado en los posts anteriores. Tendremos una primera capa `embedding` que se encargar√° de proveer a la `RNN` de la representaci√≥n vectorial de cada palabra y luego la capa `RNN` (que puede ser tambi√©n una `LSTM` o `GRU` como ya vimos en este [post](https://sensioai.com/blog/036_rnn_mejoras)). El `encoder` codificar√° la frase original y nos quedaremos con las salidas de las capas ocultas en el √∫ltimo paso (cuando ya ha visto toda la frase). Este tensor ser√° el responsable de codificar el significado de la frase para que luego el `decoder` pueda traducirla."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1686932149787,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"vDUmpIaRC_Eb"},"outputs":[],"source":["class Encoder(torch.nn.Module):\n","    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n","        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n","\n","    def forward(self, input_sentences):\n","        embedded = self.embedding(input_sentences)\n","        output, hidden = self.gru(embedded)\n","        # del encoder nos interesa el √∫ltimo *hidden state*\n","        return hidden"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1686932149787,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"uTQt4rvoC_Ec","outputId":"5bf460e1-cbb8-4574-c1bd-bebc5c9dea2b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 64, 100])"]},"metadata":{},"execution_count":16}],"source":["encoder = Encoder(input_size=input_lang.n_words)\n","hidden = encoder(torch.randint(0, input_lang.n_words, (64, 10)))\n","\n","# [num layers, batch size, hidden size]\n","hidden.shape"]},{"cell_type":"markdown","metadata":{"id":"adyZHlBFC_Ec"},"source":["### El *decoder*"]},{"cell_type":"markdown","metadata":{"id":"V8RFpDSaC_Ec"},"source":["El `decoder` ser√° de nuevo una `red neuronal recurrente`. A diferencia del `encoder`, el estado oculto del `decoder` lo inicializaremos con el tensor obtenido a la salida del `encoder`. Tanto durante el entrenamiento como en fase de inferencia, le daremos al decoder como primera palabra el token `<sos>`. Con esta informaci√≥n, y el estado oculto del `encoder`, deber√° predecir la primera palabra de la frase traducida. Seguidamente, usaremos esta primera palabra como nueva entrada junto al estado oculto obtenido en el paso anterior para generar la segunda palabra. Repetiremos este proceso hasta que el `decoder` nos de el token `<eos>`, indicando que la frase ha terminado."]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1686932149788,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"_yvRG2kXC_Ec"},"outputs":[],"source":["class Decoder(torch.nn.Module):\n","    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2):\n","        super().__init__()\n","        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n","        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n","        self.out = torch.nn.Linear(hidden_size, input_size)\n","\n","    def forward(self, input_words, hidden):\n","        embedded = self.embedding(input_words)\n","        output, hidden = self.gru(embedded, hidden)\n","        output = self.out(output.squeeze(1))\n","        return output, hidden"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1686932149788,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"Fcz74tqPC_Ec","outputId":"c3e36a6a-70b4-42d0-eb8d-ff0e3771b8d4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 26182])"]},"metadata":{},"execution_count":18}],"source":["decoder = Decoder(input_size=output_lang.n_words)\n","output, decoder_hidden = decoder(torch.randint(0, output_lang.n_words, (64, 1)), hidden)\n","\n","# [batch size, vocab size]\n","output.shape"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1686932149788,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"zIZ-DHMrC_Ed","outputId":"79d7cc02-0bea-4c1f-f556-8d6016ebec59"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 64, 100])"]},"metadata":{},"execution_count":19}],"source":["# [num layers, batch size, hidden size]\n","decoder_hidden.shape"]},{"cell_type":"markdown","metadata":{"id":"Pdnf3JvXC_Ed"},"source":["## Entrenamiento"]},{"cell_type":"markdown","metadata":{"id":"OOdgZQUCC_Ed"},"source":["Vamos a implementar el bucle de entrenamiento. En primer lugar, al tener ahora dos redes neuronales, necesitaremos dos optimizadores (uno para el `encoder` y otro para el `decoder`). Al `encoder` le pasaremos la frase en el idioma original, y obtendremos el estado oculto final. Este estado oculto lo usaremos para inicializar el `decoder` que, junto al token `<sos>`, generar√° la primera palabra de la frase traducida. Repetiremos el proceso, utilizando como entrada la anterior salida del decoder, hasta obtener el token `<eos>`."]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1686932149789,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"m_G7AD-hC_Ed"},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","\n","def fit(encoder, decoder, dataloader, epochs=10):\n","    encoder.to(device)\n","    decoder.to(device)\n","    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n","    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n","    criterion = torch.nn.CrossEntropyLoss()\n","    for epoch in range(1, epochs+1):\n","        encoder.train()\n","        decoder.train()\n","        train_loss = []\n","        bar = tqdm(dataloader['train'])\n","        for batch in bar:\n","            input_sentences, output_sentences = batch\n","            bs = input_sentences.shape[0]\n","            loss = 0\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            # obtenemos el √∫ltimo estado oculto del encoder\n","            hidden = encoder(input_sentences)\n","            # calculamos las salidas del decoder de manera recurrente\n","            decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n","            for i in range(output_sentences.shape[1]):\n","                output, hidden = decoder(decoder_input, hidden)\n","                loss += criterion(output, output_sentences[:, i].view(bs))\n","                # el siguiente input ser√° la palbra predicha\n","                decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n","            # optimizaci√≥n\n","            loss.backward()\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","            train_loss.append(loss.item())\n","            bar.set_description(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f}\")\n","\n","        val_loss = []\n","        encoder.eval()\n","        decoder.eval()\n","        with torch.no_grad():\n","            bar = tqdm(dataloader['test'])\n","            for batch in bar:\n","                input_sentences, output_sentences = batch\n","                bs = input_sentences.shape[0]\n","                loss = 0\n","                # obtenemos el √∫ltimo estado oculto del encoder\n","                hidden = encoder(input_sentences)\n","                # calculamos las salidas del decoder de manera recurrente\n","                decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n","                for i in range(output_sentences.shape[1]):\n","                    output, hidden = decoder(decoder_input, hidden)\n","                    loss += criterion(output, output_sentences[:, i].view(bs))\n","                    # el siguiente input ser√° la palbra predicha\n","                    decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n","                val_loss.append(loss.item())\n","                bar.set_description(f\"Epoch {epoch}/{epochs} val_loss {np.mean(val_loss):.5f}\")"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h_1N3aHWC_Ed","executionInfo":{"status":"ok","timestamp":1686933867595,"user_tz":240,"elapsed":1717814,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"}},"outputId":"c39c593f-b625-49bc-a8cc-26fe70bc1565"},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/30 loss 37.69305: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:52<00:00, 31.95it/s]\n","Epoch 1/30 val_loss 72.26421: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 18.47it/s]\n","Epoch 2/30 loss 29.95790: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.75it/s]\n","Epoch 2/30 val_loss 68.42555: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:06<00:00, 17.50it/s]\n","Epoch 3/30 loss 26.55357: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:50<00:00, 33.12it/s]\n","Epoch 3/30 val_loss 66.44819: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:06<00:00, 16.67it/s]\n","Epoch 4/30 loss 24.14408: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:50<00:00, 33.05it/s]\n","Epoch 4/30 val_loss 65.05403: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:06<00:00, 17.46it/s]\n","Epoch 5/30 loss 22.24489: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:52<00:00, 32.06it/s]\n","Epoch 5/30 val_loss 63.24605: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 18.04it/s]\n","Epoch 6/30 loss 20.72656: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:50<00:00, 33.09it/s]\n","Epoch 6/30 val_loss 62.79085: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 18.75it/s]\n","Epoch 7/30 loss 19.46702: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:50<00:00, 32.88it/s]\n","Epoch 7/30 val_loss 62.32363: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 18.68it/s]\n","Epoch 8/30 loss 18.42703: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.71it/s]\n","Epoch 8/30 val_loss 62.56858: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 18.92it/s]\n","Epoch 9/30 loss 17.51321: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:52<00:00, 32.24it/s]\n","Epoch 9/30 val_loss 63.32541: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:06<00:00, 17.21it/s]\n","Epoch 10/30 loss 16.72969: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:50<00:00, 32.92it/s]\n","Epoch 10/30 val_loss 62.95714: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:06<00:00, 16.89it/s]\n","Epoch 11/30 loss 16.03750: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.43it/s]\n","Epoch 11/30 val_loss 63.57241: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 17.90it/s]\n","Epoch 12/30 loss 15.43148: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:52<00:00, 31.70it/s]\n","Epoch 12/30 val_loss 63.75719: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:06<00:00, 16.11it/s]\n","Epoch 13/30 loss 14.90028: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.31it/s]\n","Epoch 13/30 val_loss 64.10619: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 18.34it/s]\n","Epoch 14/30 loss 14.41978: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:50<00:00, 33.08it/s]\n","Epoch 14/30 val_loss 64.22167: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:06<00:00, 17.30it/s]\n","Epoch 15/30 loss 13.98723: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:50<00:00, 33.22it/s]\n","Epoch 15/30 val_loss 65.69188: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:06<00:00, 16.96it/s]\n","Epoch 16/30 loss 13.57970: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.60it/s]\n","Epoch 16/30 val_loss 65.17625: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 17.99it/s]\n","Epoch 17/30 loss 13.22510: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.87it/s]\n","Epoch 17/30 val_loss 65.92206: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 18.98it/s]\n","Epoch 18/30 loss 12.89830: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.73it/s]\n","Epoch 18/30 val_loss 65.75103: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 18.88it/s]\n","Epoch 19/30 loss 12.60229: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.79it/s]\n","Epoch 19/30 val_loss 66.52507: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 18.94it/s]\n","Epoch 20/30 loss 12.32152: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.72it/s]\n","Epoch 20/30 val_loss 66.92089: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 17.95it/s]\n","Epoch 21/30 loss 12.07422: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.55it/s]\n","Epoch 21/30 val_loss 67.38679: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:06<00:00, 16.87it/s]\n","Epoch 22/30 loss 11.83905: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.38it/s]\n","Epoch 22/30 val_loss 69.07489: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:06<00:00, 17.46it/s]\n","Epoch 23/30 loss 11.60147: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.60it/s]\n","Epoch 23/30 val_loss 68.16777: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 18.98it/s]\n","Epoch 24/30 loss 11.41286: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.65it/s]\n","Epoch 24/30 val_loss 70.03828: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 18.99it/s]\n","Epoch 25/30 loss 11.20855: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.78it/s]\n","Epoch 25/30 val_loss 69.72863: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 18.46it/s]\n","Epoch 26/30 loss 11.04012: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:50<00:00, 32.90it/s]\n","Epoch 26/30 val_loss 70.36337: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:06<00:00, 17.49it/s]\n","Epoch 27/30 loss 10.87234: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.78it/s]\n","Epoch 27/30 val_loss 71.05075: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:06<00:00, 16.97it/s]\n","Epoch 28/30 loss 10.71271: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.81it/s]\n","Epoch 28/30 val_loss 70.69996: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 17.71it/s]\n","Epoch 29/30 loss 10.54739: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.30it/s]\n","Epoch 29/30 val_loss 71.76396: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 18.84it/s]\n","Epoch 30/30 loss 10.41257: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1677/1677 [00:51<00:00, 32.67it/s]\n","Epoch 30/30 val_loss 72.83676: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:05<00:00, 19.07it/s]\n"]}],"source":["fit(encoder, decoder, dataloader, epochs=30)"]},{"cell_type":"markdown","metadata":{"id":"LjwZ8FCYC_Ee"},"source":["Como puedes ver, la *loss* de enterenamiento baja indicando que nuestra red est√° aprendiendo a traducir. Sin embargo, la *loss* de validaci√≥n sube indicando que estamos haciendo *overfitting*. Esto es normal ya que estamos utilizando muy pocos datos para el entrenamiento, para reducir este problema tendr√≠as que utilizar un dataset con m√°s ejemplos."]},{"cell_type":"markdown","metadata":{"id":"1EIOmMltC_Ee"},"source":["## Generando traducciones"]},{"cell_type":"markdown","metadata":{"id":"tpa_oOF5C_Ee"},"source":["Una vez tenemos nuestro modelo entrenado, podemos utilizarlo para traducir frases del ingl√©s al castellano de la siguiente manera."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"BDq4Y7xSC_Ee","outputId":"dab9222e-5ee8-4ab5-d6d8-dae8dfa24feb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686933867596,"user_tz":240,"elapsed":17,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['be', 'good', '.', 'EOS'], ['sea', 'bueno', '.', 'EOS'])"]},"metadata":{},"execution_count":22}],"source":["input_sentence, output_sentence = dataset['train'][129]\n","input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"9fRLwXsFC_Ee","executionInfo":{"status":"ok","timestamp":1686933867596,"user_tz":240,"elapsed":6,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"}}},"outputs":[],"source":["def predict(input_sentence):\n","    # obtenemos el √∫ltimo estado oculto del encoder\n","    hidden = encoder(input_sentence.unsqueeze(0))\n","    # calculamos las salidas del decoder de manera recurrente\n","    decoder_input = torch.tensor([[output_lang.word2index['SOS']]], device=device)\n","    # iteramos hasta que el decoder nos de el token <eos>\n","    outputs = []\n","    while True:\n","        output, hidden = decoder(decoder_input, hidden)\n","        decoder_input = torch.argmax(output, axis=1).view(1, 1)\n","        outputs.append(decoder_input.cpu().item())\n","        if decoder_input.item() == output_lang.word2index['EOS']:\n","            break\n","    return output_lang.sentenceFromIndex(outputs)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"9KgVQxVbC_Ee","outputId":"61a54fa9-49fd-43ca-c685-54bf2f193236","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686933867596,"user_tz":240,"elapsed":5,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['sean', 'buenas', '.', 'EOS']"]},"metadata":{},"execution_count":24}],"source":["predict(input_sentence)"]},{"cell_type":"markdown","metadata":{"id":"I1eswXQpC_Ee"},"source":["## Resumen"]},{"cell_type":"markdown","metadata":{"id":"pV8i91ayC_Ef"},"source":["En este post hemos aprendido a implementar una arquitectura `encoder-decoder` que nos permite trabajar con secuencia de longitud arbitraria tanto en las entradas como en las salida. El ejemplo de aplicaci√≥n que hemos llevado a cabo es la traducci√≥n de texto. Esta arquitectura es muy vers√°til y puede utilizarse, con peque√±os cambios, para otras aplicaciones como generaci√≥n de descripciones a partir de im√°genes (cambiando el encoder por una red convolucional, por ejemplo). Si bien esta arquitectura nos permite obtener buenos resultados, se ve limitada en el caso en el que trabajemos con secuencias muy largas, ya que el √∫ltimo estado del `encoder` es responsable de codificar todo el significado de la frase original, lo cual puede ser dif√≠cil. Podemos mejorar esta arquitectura a√±adiendo una capa de `atenttion` en el `decoder`, el cual no solo recibir√° este estado oculto del `encoder` si no que adem√°s ser√° capaz de mirar a todas las salidas del mismo para decidir, en cada caso, la mejor palabra a traducir. En el pr√≥ximo post veremos como implementar este nuevo mecanismo."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/juansensio/blog/blob/master/040_encoder_decoder/encoder_decoder.ipynb","timestamp":1638514035330}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"233.594px"},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":0}